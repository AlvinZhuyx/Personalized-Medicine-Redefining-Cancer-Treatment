{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tqdm\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import util\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from data_preprocessing import *\n",
    "import word_embedding_load as wel\n",
    "import baseline_classification as bc\n",
    "from classification import *\n",
    "from xgboost_classifier import *\n",
    "from testaccuracy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[all_data, train_size, test_size, train_x, train_y, test_x] = util.loadData()\n",
    "sentences = data_preprocess(all_data)\n",
    "Text_INPUT_DIM=200\n",
    "GENE_INPUT_DIM=25\n",
    "TEXT_INPUT_DIM=200\n",
    "PATH = '../model/doc2vec/'\n",
    "modelName=['docEmbeddings_win2_load_all.d2v', \n",
    "           'textModel_win=2_no_outside', \n",
    "           'docEmbeddings_30_load_all.d2v', \n",
    "           'textModel_win=30_no_outside']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=GENE_INPUT_DIM, n_iter=25, random_state=12)\n",
    "truncated_one_hot_gene = wel.getGeneVec(all_data, svd)\n",
    "truncated_one_hot_variation = wel.getVariationVec(all_data, svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_y = pd.get_dummies(train_y)\n",
    "encoded_y = np.array(encoded_y)\n",
    "y = np.array(bc.getLabels(encoded_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3689, 9)\n",
      "[1 2 2 3 4 4 5 1 4]\n",
      "(3689,)\n",
      "[[0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_y.shape)\n",
    "print(train_y[0:9])\n",
    "print(y.shape)\n",
    "print(encoded_y[[1,3,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "import util\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "def nn_cross_validation(X, y, encoded_y, skf = StratifiedKFold(n_splits=10, random_state = 66, shuffle = True)):\n",
    "    ret = []\n",
    "    mat = 0\n",
    "    LOSS = []\n",
    "    ACC = []\n",
    "    NMI = []\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = encoded_y[train_index], encoded_y[test_index]\n",
    "        model = nn_baseline_model(25, 25, 200)\n",
    "        model = train_nn_model_simple(model, X_train, y_train, 'temp.h5')\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_pred_proba, axis = 1)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        acc = accuracy_score(y[test_index], y_pred)\n",
    "        nmi = normalized_mutual_info_score(y_pred, y[test_index])\n",
    "        loss = log_loss(y[test_index], y_pred_proba)\n",
    "        ACC.append(acc)\n",
    "        LOSS.append(loss)\n",
    "        NMI.append(nmi)\n",
    "        cnf_matrix = confusion_matrix(y[test_index], y_pred)\n",
    "        mat = mat + cnf_matrix\n",
    "    print(\"Accuracy: %.4f ± %.4f\" % (np.mean(ACC), np.std(ACC)))\n",
    "    print(\"NMI: %.4f ± %.4f\" % (np.mean(NMI), np.std(NMI)))\n",
    "    print(\"Log_loss: %.4f ± %.4f\" % (np.mean(LOSS), np.std(LOSS)))\n",
    "    mat = mat / skf.get_n_splits()\n",
    "    mat = np.array(mat)\n",
    "    mat = mat.astype('float') / mat.sum(axis=1)[:, np.newaxis]\n",
    "    util.plot_confusion_matrix(mat, classes='', normalize=True,\n",
    "                      title='Confusion matrix')\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nn_model_simple(model, train_set, encoded_y, filename = 'best_weight_predict_all.h5'):\n",
    "    print('begin training\\n')\n",
    "    best_acc = 0\n",
    "    acc = []\n",
    "    val_acc = []\n",
    "    loss = []\n",
    "    val_loss = []\n",
    "    for i in range(30):\n",
    "        estimator=model.fit(train_set, encoded_y, validation_split=0.01, epochs=2, batch_size=64)\n",
    "        if (best_acc < estimator.history['val_acc'][-1] * 100):\n",
    "            best_acc = estimator.history['val_acc'][-1] * 100\n",
    "            model.save_weights(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the textmodel from ../model/doc2vec/docEmbeddings_win2_load_all.d2v\n",
      "Use test model: docEmbeddings_win2_load_all.d2v\n",
      "begin training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/qqu0127/Desktop/CS249/master/Personalized-Medicine-Redefining-Cancer-Treatment/src/classification.py:28: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, input_dim=250, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  model.add(Dense(256, input_dim=Text_INPUT_DIM+ Gene_INPUT_DIM + Variation_INPUT_DIM, init='normal', activation='relu'))\n",
      "/Users/qqu0127/Desktop/CS249/master/Personalized-Medicine-Redefining-Cancer-Treatment/src/classification.py:30: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(256, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  model.add(Dense(256, init='normal', activation='relu'))\n",
      "/Users/qqu0127/Desktop/CS249/master/Personalized-Medicine-Redefining-Cancer-Treatment/src/classification.py:32: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(80, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  model.add(Dense(80, init='normal', activation='relu'))\n",
      "/Users/qqu0127/Desktop/CS249/master/Personalized-Medicine-Redefining-Cancer-Treatment/src/classification.py:33: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(9, activation=\"softmax\", kernel_initializer=\"normal\")`\n",
      "  model.add(Dense(9, init='normal', activation=\"softmax\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 1s 285us/step - loss: 1.9869 - acc: 0.2575 - val_loss: 1.5532 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 60us/step - loss: 1.6185 - acc: 0.4224 - val_loss: 1.2641 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 60us/step - loss: 1.4198 - acc: 0.4706 - val_loss: 1.0899 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.3238 - acc: 0.5084 - val_loss: 0.9800 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 60us/step - loss: 1.2537 - acc: 0.5468 - val_loss: 0.9253 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 1.1905 - acc: 0.5544 - val_loss: 0.8993 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.1578 - acc: 0.5834 - val_loss: 0.8455 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 1.1207 - acc: 0.5904 - val_loss: 0.8054 - val_acc: 0.7941\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.0852 - acc: 0.5943 - val_loss: 0.8290 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0591 - acc: 0.6047 - val_loss: 0.8066 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.0516 - acc: 0.6077 - val_loss: 0.7947 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0242 - acc: 0.6169 - val_loss: 0.7566 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0018 - acc: 0.6202 - val_loss: 0.7665 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.9788 - acc: 0.6285 - val_loss: 0.7374 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.9811 - acc: 0.6376 - val_loss: 0.7810 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.9512 - acc: 0.6538 - val_loss: 0.7380 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 76us/step - loss: 0.9329 - acc: 0.6532 - val_loss: 0.7647 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 77us/step - loss: 0.9204 - acc: 0.6614 - val_loss: 0.7681 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.9004 - acc: 0.6681 - val_loss: 0.7526 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.8915 - acc: 0.6660 - val_loss: 0.7537 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 70us/step - loss: 0.8789 - acc: 0.6781 - val_loss: 0.7557 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.8622 - acc: 0.6748 - val_loss: 0.7688 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.8580 - acc: 0.6839 - val_loss: 0.7387 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 0.8509 - acc: 0.6937 - val_loss: 0.7790 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.8348 - acc: 0.6900 - val_loss: 0.7284 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 73us/step - loss: 0.8251 - acc: 0.6845 - val_loss: 0.7616 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.8150 - acc: 0.6970 - val_loss: 0.7509 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.8068 - acc: 0.6986 - val_loss: 0.7378 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.7989 - acc: 0.6967 - val_loss: 0.7397 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 73us/step - loss: 0.7942 - acc: 0.6919 - val_loss: 0.7574 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.7790 - acc: 0.7034 - val_loss: 0.7677 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.7725 - acc: 0.7028 - val_loss: 0.7488 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.7671 - acc: 0.7083 - val_loss: 0.7700 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.7561 - acc: 0.7165 - val_loss: 0.7666 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7517 - acc: 0.7105 - val_loss: 0.7514 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7414 - acc: 0.7217 - val_loss: 0.7479 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.7301 - acc: 0.7101 - val_loss: 0.7625 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 77us/step - loss: 0.7289 - acc: 0.7211 - val_loss: 0.7452 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7298 - acc: 0.7248 - val_loss: 0.7354 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.7045 - acc: 0.7205 - val_loss: 0.7510 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7180 - acc: 0.7309 - val_loss: 0.7768 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6999 - acc: 0.7284 - val_loss: 0.7682 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6943 - acc: 0.7312 - val_loss: 0.7781 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6800 - acc: 0.7391 - val_loss: 0.8099 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6966 - acc: 0.7306 - val_loss: 0.7773 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6646 - acc: 0.7403 - val_loss: 0.7910 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6655 - acc: 0.7428 - val_loss: 0.7706 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6564 - acc: 0.7412 - val_loss: 0.7846 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6437 - acc: 0.7556 - val_loss: 0.7980 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6541 - acc: 0.7440 - val_loss: 0.7886 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6393 - acc: 0.7546 - val_loss: 0.7911 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6410 - acc: 0.7486 - val_loss: 0.7729 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6341 - acc: 0.7546 - val_loss: 0.7874 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6328 - acc: 0.7528 - val_loss: 0.7930 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6238 - acc: 0.7607 - val_loss: 0.7893 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 60us/step - loss: 0.6147 - acc: 0.7610 - val_loss: 0.7951 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6075 - acc: 0.7638 - val_loss: 0.8211 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6185 - acc: 0.7577 - val_loss: 0.7955 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 61us/step - loss: 0.6091 - acc: 0.7668 - val_loss: 0.8053 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - ETA: 0s - loss: 0.5971 - acc: 0.767 - 0s 60us/step - loss: 0.6032 - acc: 0.7684 - val_loss: 0.8313 - val_acc: 0.6176\n",
      "begin training\n",
      "\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 1s 292us/step - loss: 1.9627 - acc: 0.2595 - val_loss: 1.6155 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 1.6210 - acc: 0.4045 - val_loss: 1.3265 - val_acc: 0.5294\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 60us/step - loss: 1.3964 - acc: 0.4895 - val_loss: 1.0965 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 1.2932 - acc: 0.5309 - val_loss: 1.0168 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 60us/step - loss: 1.2173 - acc: 0.5504 - val_loss: 0.9329 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 1.1776 - acc: 0.5687 - val_loss: 0.9166 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 1.1288 - acc: 0.5769 - val_loss: 0.8573 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 1.0902 - acc: 0.5879 - val_loss: 0.8580 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 60us/step - loss: 1.0819 - acc: 0.6019 - val_loss: 0.8228 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 1.0415 - acc: 0.6186 - val_loss: 0.8208 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 1.0286 - acc: 0.6156 - val_loss: 0.8543 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.9947 - acc: 0.6333 - val_loss: 0.7940 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.9759 - acc: 0.6360 - val_loss: 0.7911 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.9741 - acc: 0.6439 - val_loss: 0.7835 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.9576 - acc: 0.6454 - val_loss: 0.8287 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.9383 - acc: 0.6564 - val_loss: 0.8275 - val_acc: 0.5882\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.9315 - acc: 0.6573 - val_loss: 0.8297 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.9078 - acc: 0.6607 - val_loss: 0.8368 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.8956 - acc: 0.6747 - val_loss: 0.8014 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8780 - acc: 0.6768 - val_loss: 0.7976 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8754 - acc: 0.6817 - val_loss: 0.8502 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.8536 - acc: 0.6866 - val_loss: 0.8432 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8475 - acc: 0.6896 - val_loss: 0.8281 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.8427 - acc: 0.6869 - val_loss: 0.8029 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 0.8326 - acc: 0.6814 - val_loss: 0.8076 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8053 - acc: 0.7070 - val_loss: 0.8424 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.8066 - acc: 0.6957 - val_loss: 0.8206 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.8115 - acc: 0.6978 - val_loss: 0.8284 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 76us/step - loss: 0.7961 - acc: 0.6981 - val_loss: 0.8049 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.7907 - acc: 0.7048 - val_loss: 0.7876 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.7810 - acc: 0.7055 - val_loss: 0.7965 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7597 - acc: 0.7225 - val_loss: 0.8300 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.7455 - acc: 0.7210 - val_loss: 0.8157 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 0.7584 - acc: 0.7219 - val_loss: 0.7852 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.7438 - acc: 0.7131 - val_loss: 0.7843 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 76us/step - loss: 0.7436 - acc: 0.7216 - val_loss: 0.8035 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.7257 - acc: 0.7301 - val_loss: 0.8076 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.7211 - acc: 0.7277 - val_loss: 0.7764 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.7116 - acc: 0.7283 - val_loss: 0.7472 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 0.6856 - acc: 0.7420 - val_loss: 0.8072 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.6871 - acc: 0.7454 - val_loss: 0.7978 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6796 - acc: 0.7399 - val_loss: 0.7961 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.6886 - acc: 0.7307 - val_loss: 0.8474 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.6880 - acc: 0.7438 - val_loss: 0.8025 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.6543 - acc: 0.7518 - val_loss: 0.8405 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 0.6717 - acc: 0.7496 - val_loss: 0.7723 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.6665 - acc: 0.7481 - val_loss: 0.8169 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 0.6462 - acc: 0.7511 - val_loss: 0.8067 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 0.6429 - acc: 0.7493 - val_loss: 0.7978 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.6363 - acc: 0.7569 - val_loss: 0.8056 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 60us/step - loss: 0.6272 - acc: 0.7618 - val_loss: 0.8203 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.6158 - acc: 0.7636 - val_loss: 0.8400 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.6198 - acc: 0.7560 - val_loss: 0.7821 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 60us/step - loss: 0.6117 - acc: 0.7648 - val_loss: 0.8021 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.6160 - acc: 0.7572 - val_loss: 0.8128 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.6072 - acc: 0.7642 - val_loss: 0.8078 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 61us/step - loss: 0.6067 - acc: 0.7676 - val_loss: 0.8127 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6068 - acc: 0.7679 - val_loss: 0.8264 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.5993 - acc: 0.7749 - val_loss: 0.8410 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 62us/step - loss: 0.5761 - acc: 0.7840 - val_loss: 0.8134 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 1s 327us/step - loss: 1.9780 - acc: 0.2521 - val_loss: 1.6387 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 80us/step - loss: 1.6773 - acc: 0.3946 - val_loss: 1.3827 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 1.4663 - acc: 0.4650 - val_loss: 1.1884 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 1.3574 - acc: 0.4979 - val_loss: 1.1306 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.2727 - acc: 0.5311 - val_loss: 0.9893 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.2270 - acc: 0.5515 - val_loss: 1.0142 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 1.1774 - acc: 0.5621 - val_loss: 0.9268 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 1.1362 - acc: 0.5752 - val_loss: 0.9073 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.1159 - acc: 0.5859 - val_loss: 0.9075 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 1.0746 - acc: 0.6044 - val_loss: 0.8860 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 1.0502 - acc: 0.6118 - val_loss: 0.9014 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.0298 - acc: 0.6175 - val_loss: 0.9054 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 1.0143 - acc: 0.6306 - val_loss: 0.9246 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.9995 - acc: 0.6361 - val_loss: 0.8657 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.9927 - acc: 0.6398 - val_loss: 0.8679 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.9636 - acc: 0.6404 - val_loss: 0.8541 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.9592 - acc: 0.6422 - val_loss: 0.8788 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 0.9394 - acc: 0.6562 - val_loss: 0.8500 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.9204 - acc: 0.6602 - val_loss: 0.8849 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 0.9035 - acc: 0.6702 - val_loss: 0.8669 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 0.9087 - acc: 0.6660 - val_loss: 0.9010 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.8835 - acc: 0.6757 - val_loss: 0.9077 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.8620 - acc: 0.6751 - val_loss: 0.8829 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.8698 - acc: 0.6772 - val_loss: 0.8539 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.8451 - acc: 0.6882 - val_loss: 0.8829 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 0.8459 - acc: 0.6781 - val_loss: 0.8936 - val_acc: 0.5000\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.8327 - acc: 0.6891 - val_loss: 0.8515 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 0.8157 - acc: 0.6924 - val_loss: 0.8538 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 0.8204 - acc: 0.6967 - val_loss: 0.8527 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7975 - acc: 0.7034 - val_loss: 0.8699 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.8067 - acc: 0.6937 - val_loss: 0.8440 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 61us/step - loss: 0.7722 - acc: 0.7089 - val_loss: 0.8436 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7779 - acc: 0.7046 - val_loss: 0.8408 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.7770 - acc: 0.7107 - val_loss: 0.8191 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 0.7685 - acc: 0.7101 - val_loss: 0.8395 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.7359 - acc: 0.7229 - val_loss: 0.8246 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.7322 - acc: 0.7220 - val_loss: 0.8531 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.7519 - acc: 0.7080 - val_loss: 0.8427 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 0.7412 - acc: 0.7278 - val_loss: 0.8003 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.7097 - acc: 0.7256 - val_loss: 0.8220 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.7180 - acc: 0.7305 - val_loss: 0.8395 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.6934 - acc: 0.7406 - val_loss: 0.8500 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 0.7033 - acc: 0.7293 - val_loss: 0.8607 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.6923 - acc: 0.7396 - val_loss: 0.8316 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 61us/step - loss: 0.6879 - acc: 0.7400 - val_loss: 0.8472 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6796 - acc: 0.7375 - val_loss: 0.8686 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.6783 - acc: 0.7503 - val_loss: 0.8250 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.6778 - acc: 0.7491 - val_loss: 0.8657 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.6599 - acc: 0.7552 - val_loss: 0.8621 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.6675 - acc: 0.7530 - val_loss: 0.8498 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6443 - acc: 0.7540 - val_loss: 0.8290 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.6527 - acc: 0.7512 - val_loss: 0.8414 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6404 - acc: 0.7549 - val_loss: 0.8147 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 0.6404 - acc: 0.7503 - val_loss: 0.8288 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.6397 - acc: 0.7634 - val_loss: 0.8795 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.6189 - acc: 0.7661 - val_loss: 0.8580 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.6068 - acc: 0.7649 - val_loss: 0.8806 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6177 - acc: 0.7573 - val_loss: 0.8585 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6055 - acc: 0.7628 - val_loss: 0.8805 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 0.5981 - acc: 0.7704 - val_loss: 0.9057 - val_acc: 0.5588\n",
      "begin training\n",
      "\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 1s 340us/step - loss: 1.9597 - acc: 0.2992 - val_loss: 1.5365 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 1.5982 - acc: 0.4140 - val_loss: 1.2262 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 1.4203 - acc: 0.4834 - val_loss: 1.1540 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 1.3291 - acc: 0.5202 - val_loss: 0.9977 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 1.2451 - acc: 0.5458 - val_loss: 0.8860 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 1.1917 - acc: 0.5683 - val_loss: 0.8695 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 74us/step - loss: 1.1610 - acc: 0.5769 - val_loss: 0.8453 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 75us/step - loss: 1.1298 - acc: 0.5781 - val_loss: 0.8541 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 1.0964 - acc: 0.5942 - val_loss: 0.8419 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0722 - acc: 0.5991 - val_loss: 0.8347 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 1.0575 - acc: 0.6122 - val_loss: 0.8308 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0360 - acc: 0.6253 - val_loss: 0.8620 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.0167 - acc: 0.6307 - val_loss: 0.8224 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 1.0060 - acc: 0.6359 - val_loss: 0.8205 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.9731 - acc: 0.6429 - val_loss: 0.7861 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.9508 - acc: 0.6490 - val_loss: 0.8141 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.9429 - acc: 0.6548 - val_loss: 0.8343 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.9364 - acc: 0.6545 - val_loss: 0.8180 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.9099 - acc: 0.6654 - val_loss: 0.8186 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.9074 - acc: 0.6603 - val_loss: 0.8270 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.8946 - acc: 0.6807 - val_loss: 0.8450 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.8876 - acc: 0.6658 - val_loss: 0.8203 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.8666 - acc: 0.6834 - val_loss: 0.8418 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.8564 - acc: 0.6846 - val_loss: 0.8494 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8380 - acc: 0.6855 - val_loss: 0.8395 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.8464 - acc: 0.6886 - val_loss: 0.8577 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8383 - acc: 0.6919 - val_loss: 0.8355 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.8282 - acc: 0.6846 - val_loss: 0.8070 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.8197 - acc: 0.7011 - val_loss: 0.8515 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.7932 - acc: 0.6983 - val_loss: 0.8676 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.7994 - acc: 0.6983 - val_loss: 0.8649 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.7840 - acc: 0.6989 - val_loss: 0.8556 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.7631 - acc: 0.7172 - val_loss: 0.8301 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.7691 - acc: 0.7111 - val_loss: 0.8335 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.7653 - acc: 0.7160 - val_loss: 0.8265 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.7473 - acc: 0.7209 - val_loss: 0.8377 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.7572 - acc: 0.7163 - val_loss: 0.8436 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.7391 - acc: 0.7212 - val_loss: 0.8443 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.7200 - acc: 0.7306 - val_loss: 0.8392 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.7265 - acc: 0.7272 - val_loss: 0.8712 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.7091 - acc: 0.7373 - val_loss: 0.8714 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.7093 - acc: 0.7282 - val_loss: 0.8315 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.6895 - acc: 0.7397 - val_loss: 0.8583 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.6920 - acc: 0.7376 - val_loss: 0.8394 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6926 - acc: 0.7318 - val_loss: 0.8997 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.6732 - acc: 0.7482 - val_loss: 0.8805 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.6693 - acc: 0.7434 - val_loss: 0.8613 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6676 - acc: 0.7440 - val_loss: 0.8611 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6615 - acc: 0.7577 - val_loss: 0.8821 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.6502 - acc: 0.7495 - val_loss: 0.8385 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.6557 - acc: 0.7577 - val_loss: 0.8649 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6391 - acc: 0.7662 - val_loss: 0.8536 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6311 - acc: 0.7650 - val_loss: 0.8562 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6296 - acc: 0.7592 - val_loss: 0.8845 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6206 - acc: 0.7571 - val_loss: 0.9153 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 62us/step - loss: 0.6160 - acc: 0.7686 - val_loss: 0.8604 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.6078 - acc: 0.7723 - val_loss: 0.8894 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 60us/step - loss: 0.6112 - acc: 0.7723 - val_loss: 0.8787 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.6182 - acc: 0.7638 - val_loss: 0.9282 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 61us/step - loss: 0.5958 - acc: 0.7668 - val_loss: 0.9302 - val_acc: 0.6176\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 1s 328us/step - loss: 1.9426 - acc: 0.2903 - val_loss: 1.6202 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 1.6241 - acc: 0.4172 - val_loss: 1.3887 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.4251 - acc: 0.4836 - val_loss: 1.1421 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.3199 - acc: 0.5186 - val_loss: 1.0108 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.2392 - acc: 0.5429 - val_loss: 0.9304 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.1747 - acc: 0.5636 - val_loss: 0.9013 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.1372 - acc: 0.5736 - val_loss: 0.8628 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.0971 - acc: 0.5922 - val_loss: 0.8614 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.0760 - acc: 0.6108 - val_loss: 0.8702 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.0466 - acc: 0.6065 - val_loss: 0.8334 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 1.0235 - acc: 0.6138 - val_loss: 0.8593 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 1.0097 - acc: 0.6284 - val_loss: 0.8493 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.9806 - acc: 0.6278 - val_loss: 0.8237 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.9640 - acc: 0.6403 - val_loss: 0.8094 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.9581 - acc: 0.6400 - val_loss: 0.8186 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.9417 - acc: 0.6467 - val_loss: 0.8093 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.9267 - acc: 0.6567 - val_loss: 0.7934 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9093 - acc: 0.6625 - val_loss: 0.8221 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.8893 - acc: 0.6646 - val_loss: 0.8292 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.8735 - acc: 0.6692 - val_loss: 0.8122 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.8691 - acc: 0.6716 - val_loss: 0.8114 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.8534 - acc: 0.6732 - val_loss: 0.8039 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.8450 - acc: 0.6823 - val_loss: 0.7932 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.8248 - acc: 0.6917 - val_loss: 0.7689 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 0.8198 - acc: 0.6911 - val_loss: 0.8278 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8088 - acc: 0.6878 - val_loss: 0.8099 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.8236 - acc: 0.6911 - val_loss: 0.8039 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.8033 - acc: 0.6963 - val_loss: 0.8150 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.8081 - acc: 0.6948 - val_loss: 0.7796 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7759 - acc: 0.7142 - val_loss: 0.8195 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7608 - acc: 0.7121 - val_loss: 0.8262 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.7636 - acc: 0.7030 - val_loss: 0.8310 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7638 - acc: 0.7106 - val_loss: 0.7862 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7417 - acc: 0.7179 - val_loss: 0.7574 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7360 - acc: 0.7228 - val_loss: 0.7848 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.7323 - acc: 0.7264 - val_loss: 0.8012 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 0.7321 - acc: 0.7237 - val_loss: 0.7793 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.7063 - acc: 0.7392 - val_loss: 0.7563 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7178 - acc: 0.7313 - val_loss: 0.7638 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7070 - acc: 0.7319 - val_loss: 0.7586 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6856 - acc: 0.7425 - val_loss: 0.7914 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6896 - acc: 0.7346 - val_loss: 0.7739 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6767 - acc: 0.7477 - val_loss: 0.7716 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6888 - acc: 0.7349 - val_loss: 0.7397 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6739 - acc: 0.7422 - val_loss: 0.7487 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6718 - acc: 0.7416 - val_loss: 0.7397 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6611 - acc: 0.7502 - val_loss: 0.7242 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6418 - acc: 0.7544 - val_loss: 0.7281 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.6470 - acc: 0.7532 - val_loss: 0.7830 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 0.6398 - acc: 0.7572 - val_loss: 0.7660 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.6230 - acc: 0.7605 - val_loss: 0.7155 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6342 - acc: 0.7483 - val_loss: 0.7343 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6321 - acc: 0.7587 - val_loss: 0.7696 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6153 - acc: 0.7572 - val_loss: 0.7527 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.6155 - acc: 0.7687 - val_loss: 0.7097 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.6123 - acc: 0.7681 - val_loss: 0.7546 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6085 - acc: 0.7681 - val_loss: 0.7062 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6003 - acc: 0.7681 - val_loss: 0.8039 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 0.6036 - acc: 0.7596 - val_loss: 0.7674 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.5844 - acc: 0.7763 - val_loss: 0.7314 - val_acc: 0.7353\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 1s 359us/step - loss: 1.9195 - acc: 0.2952 - val_loss: 1.5431 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.6311 - acc: 0.4121 - val_loss: 1.2786 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 1.4284 - acc: 0.4757 - val_loss: 1.2012 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 80us/step - loss: 1.3277 - acc: 0.5125 - val_loss: 1.0555 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 1.2530 - acc: 0.5453 - val_loss: 1.0280 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.2052 - acc: 0.5584 - val_loss: 0.9339 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.1486 - acc: 0.5740 - val_loss: 0.9364 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.1148 - acc: 0.5904 - val_loss: 0.9148 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0984 - acc: 0.5886 - val_loss: 0.8629 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 1.0737 - acc: 0.6029 - val_loss: 0.8803 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.0493 - acc: 0.6166 - val_loss: 0.8697 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 1.0267 - acc: 0.6284 - val_loss: 0.8923 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 63us/step - loss: 1.0112 - acc: 0.6226 - val_loss: 0.8895 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 1.0001 - acc: 0.6287 - val_loss: 0.8544 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.9729 - acc: 0.6400 - val_loss: 0.8934 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.9694 - acc: 0.6488 - val_loss: 0.8666 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 0.9464 - acc: 0.6522 - val_loss: 0.8393 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 0.9305 - acc: 0.6586 - val_loss: 0.8525 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9178 - acc: 0.6662 - val_loss: 0.8319 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.9026 - acc: 0.6671 - val_loss: 0.8405 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8965 - acc: 0.6738 - val_loss: 0.8557 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8804 - acc: 0.6802 - val_loss: 0.8329 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8749 - acc: 0.6756 - val_loss: 0.8163 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8660 - acc: 0.6859 - val_loss: 0.8733 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8465 - acc: 0.6853 - val_loss: 0.8441 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8291 - acc: 0.6826 - val_loss: 0.8715 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 62us/step - loss: 0.8266 - acc: 0.6920 - val_loss: 0.8349 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.8213 - acc: 0.6951 - val_loss: 0.8520 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 61us/step - loss: 0.8142 - acc: 0.7069 - val_loss: 0.8652 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.8086 - acc: 0.6969 - val_loss: 0.8665 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7973 - acc: 0.7005 - val_loss: 0.8860 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7854 - acc: 0.7018 - val_loss: 0.8795 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7776 - acc: 0.7197 - val_loss: 0.8834 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7656 - acc: 0.7088 - val_loss: 0.8579 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7604 - acc: 0.7209 - val_loss: 0.8743 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.7566 - acc: 0.7115 - val_loss: 0.8131 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7361 - acc: 0.7225 - val_loss: 0.8754 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7317 - acc: 0.7282 - val_loss: 0.8535 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7366 - acc: 0.7167 - val_loss: 0.8545 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7301 - acc: 0.7231 - val_loss: 0.8297 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7259 - acc: 0.7295 - val_loss: 0.8735 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7137 - acc: 0.7422 - val_loss: 0.8165 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7010 - acc: 0.7355 - val_loss: 0.8104 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6947 - acc: 0.7304 - val_loss: 0.8200 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6869 - acc: 0.7407 - val_loss: 0.8408 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6913 - acc: 0.7346 - val_loss: 0.8638 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6699 - acc: 0.7477 - val_loss: 0.8313 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.6847 - acc: 0.7468 - val_loss: 0.8115 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6587 - acc: 0.7529 - val_loss: 0.8124 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6595 - acc: 0.7502 - val_loss: 0.8400 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6584 - acc: 0.7422 - val_loss: 0.8362 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6417 - acc: 0.7587 - val_loss: 0.8544 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6326 - acc: 0.7547 - val_loss: 0.8251 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6453 - acc: 0.7511 - val_loss: 0.8520 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6240 - acc: 0.7556 - val_loss: 0.8746 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6305 - acc: 0.7645 - val_loss: 0.8799 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6235 - acc: 0.7660 - val_loss: 0.8448 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6054 - acc: 0.7660 - val_loss: 0.8714 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6080 - acc: 0.7684 - val_loss: 0.8686 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6035 - acc: 0.7708 - val_loss: 0.8597 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 1s 366us/step - loss: 1.9242 - acc: 0.2911 - val_loss: 1.5029 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 1.5848 - acc: 0.4332 - val_loss: 1.2159 - val_acc: 0.5588\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 1.3982 - acc: 0.4874 - val_loss: 1.1345 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 1.2877 - acc: 0.5199 - val_loss: 0.9563 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 62us/step - loss: 1.2380 - acc: 0.5394 - val_loss: 0.9572 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 1.1949 - acc: 0.5513 - val_loss: 0.8722 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 1.1407 - acc: 0.5710 - val_loss: 0.8703 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 1.1212 - acc: 0.5844 - val_loss: 0.8077 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 1.1010 - acc: 0.5978 - val_loss: 0.8387 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 1.0781 - acc: 0.6066 - val_loss: 0.8205 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 1.0366 - acc: 0.6091 - val_loss: 0.8024 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 1.0193 - acc: 0.6209 - val_loss: 0.7946 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 1.0038 - acc: 0.6340 - val_loss: 0.8009 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 0.9762 - acc: 0.6416 - val_loss: 0.8021 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 0.9770 - acc: 0.6355 - val_loss: 0.8133 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 0.9540 - acc: 0.6489 - val_loss: 0.7863 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.9399 - acc: 0.6520 - val_loss: 0.8106 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.9191 - acc: 0.6611 - val_loss: 0.7857 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.9093 - acc: 0.6611 - val_loss: 0.7734 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.8892 - acc: 0.6745 - val_loss: 0.7925 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.8812 - acc: 0.6772 - val_loss: 0.7876 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.8739 - acc: 0.6793 - val_loss: 0.7849 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 89us/step - loss: 0.8688 - acc: 0.6742 - val_loss: 0.7487 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.8413 - acc: 0.6882 - val_loss: 0.7630 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.8402 - acc: 0.6876 - val_loss: 0.7668 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.8306 - acc: 0.6933 - val_loss: 0.7530 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 86us/step - loss: 0.8301 - acc: 0.6903 - val_loss: 0.7334 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.8062 - acc: 0.6955 - val_loss: 0.7599 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 77us/step - loss: 0.8079 - acc: 0.6885 - val_loss: 0.7477 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 90us/step - loss: 0.8059 - acc: 0.6943 - val_loss: 0.7624 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 78us/step - loss: 0.7912 - acc: 0.7003 - val_loss: 0.7679 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 114us/step - loss: 0.7784 - acc: 0.7085 - val_loss: 0.7278 - val_acc: 0.5882\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.7791 - acc: 0.7012 - val_loss: 0.7298 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 76us/step - loss: 0.7585 - acc: 0.7119 - val_loss: 0.7160 - val_acc: 0.5882\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7568 - acc: 0.7152 - val_loss: 0.7181 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 61us/step - loss: 0.7478 - acc: 0.7265 - val_loss: 0.6995 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.7317 - acc: 0.7286 - val_loss: 0.7155 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7190 - acc: 0.7274 - val_loss: 0.6932 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7335 - acc: 0.7274 - val_loss: 0.7263 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7209 - acc: 0.7298 - val_loss: 0.6906 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7150 - acc: 0.7335 - val_loss: 0.7133 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.7181 - acc: 0.7295 - val_loss: 0.7122 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.7015 - acc: 0.7286 - val_loss: 0.7072 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6858 - acc: 0.7408 - val_loss: 0.7093 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.6858 - acc: 0.7402 - val_loss: 0.7134 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.6762 - acc: 0.7359 - val_loss: 0.6915 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.6812 - acc: 0.7387 - val_loss: 0.6998 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.6597 - acc: 0.7475 - val_loss: 0.6889 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.6529 - acc: 0.7511 - val_loss: 0.6994 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.6618 - acc: 0.7448 - val_loss: 0.6831 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.6397 - acc: 0.7566 - val_loss: 0.6662 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.6513 - acc: 0.7426 - val_loss: 0.6853 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.6474 - acc: 0.7496 - val_loss: 0.6711 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 62us/step - loss: 0.6198 - acc: 0.7597 - val_loss: 0.6706 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.6268 - acc: 0.7557 - val_loss: 0.7067 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.6181 - acc: 0.7679 - val_loss: 0.7025 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.6087 - acc: 0.7591 - val_loss: 0.6834 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.5994 - acc: 0.7688 - val_loss: 0.6944 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 89us/step - loss: 0.6016 - acc: 0.7630 - val_loss: 0.6467 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.5969 - acc: 0.7667 - val_loss: 0.6703 - val_acc: 0.6176\n",
      "begin training\n",
      "\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 1s 365us/step - loss: 1.9177 - acc: 0.3171 - val_loss: 1.4587 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 1.5701 - acc: 0.4354 - val_loss: 1.1344 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 63us/step - loss: 1.4000 - acc: 0.4907 - val_loss: 0.9887 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 62us/step - loss: 1.3040 - acc: 0.5324 - val_loss: 0.9251 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 62us/step - loss: 1.2384 - acc: 0.5409 - val_loss: 0.8792 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 62us/step - loss: 1.1961 - acc: 0.5607 - val_loss: 0.8461 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 62us/step - loss: 1.1645 - acc: 0.5719 - val_loss: 0.8619 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 61us/step - loss: 1.1337 - acc: 0.5877 - val_loss: 0.7890 - val_acc: 0.5882\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 62us/step - loss: 1.0894 - acc: 0.5968 - val_loss: 0.7873 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 61us/step - loss: 1.0827 - acc: 0.6044 - val_loss: 0.7845 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 1.0525 - acc: 0.6011 - val_loss: 0.7907 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 1.0338 - acc: 0.6163 - val_loss: 0.7482 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 1.0170 - acc: 0.6269 - val_loss: 0.7799 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 90us/step - loss: 0.9889 - acc: 0.6379 - val_loss: 0.7744 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 91us/step - loss: 0.9777 - acc: 0.6370 - val_loss: 0.7616 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 0.9618 - acc: 0.6333 - val_loss: 0.7598 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.9336 - acc: 0.6461 - val_loss: 0.7290 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 85us/step - loss: 0.9350 - acc: 0.6516 - val_loss: 0.7831 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 103us/step - loss: 0.9138 - acc: 0.6598 - val_loss: 0.7427 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8913 - acc: 0.6692 - val_loss: 0.7458 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 87us/step - loss: 0.8807 - acc: 0.6719 - val_loss: 0.7489 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8832 - acc: 0.6665 - val_loss: 0.7290 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 63us/step - loss: 0.8640 - acc: 0.6856 - val_loss: 0.7453 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 76us/step - loss: 0.8576 - acc: 0.6914 - val_loss: 0.7855 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 77us/step - loss: 0.8450 - acc: 0.6890 - val_loss: 0.7638 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.8191 - acc: 0.6868 - val_loss: 0.8039 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.8251 - acc: 0.6963 - val_loss: 0.7889 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.8023 - acc: 0.7026 - val_loss: 0.7497 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.8030 - acc: 0.7048 - val_loss: 0.7843 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 81us/step - loss: 0.7985 - acc: 0.6963 - val_loss: 0.7540 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.7810 - acc: 0.7093 - val_loss: 0.7830 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.7733 - acc: 0.7130 - val_loss: 0.7939 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 99us/step - loss: 0.7784 - acc: 0.7057 - val_loss: 0.7707 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7565 - acc: 0.7151 - val_loss: 0.8140 - val_acc: 0.5882\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 84us/step - loss: 0.7555 - acc: 0.7136 - val_loss: 0.7779 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.7522 - acc: 0.7136 - val_loss: 0.7872 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.7520 - acc: 0.7124 - val_loss: 0.7538 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.7337 - acc: 0.7163 - val_loss: 0.7603 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.7251 - acc: 0.7182 - val_loss: 0.7918 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.7277 - acc: 0.7291 - val_loss: 0.8247 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.7133 - acc: 0.7258 - val_loss: 0.8271 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 76us/step - loss: 0.6975 - acc: 0.7358 - val_loss: 0.7854 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.6925 - acc: 0.7315 - val_loss: 0.7804 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6812 - acc: 0.7376 - val_loss: 0.8107 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.6738 - acc: 0.7449 - val_loss: 0.7772 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6764 - acc: 0.7437 - val_loss: 0.8291 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.6691 - acc: 0.7397 - val_loss: 0.8149 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.6547 - acc: 0.7440 - val_loss: 0.7917 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.6498 - acc: 0.7492 - val_loss: 0.8562 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.6451 - acc: 0.7513 - val_loss: 0.8486 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6480 - acc: 0.7589 - val_loss: 0.8309 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.6393 - acc: 0.7549 - val_loss: 0.8096 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.6419 - acc: 0.7510 - val_loss: 0.8447 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6251 - acc: 0.7559 - val_loss: 0.8394 - val_acc: 0.5882\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 97us/step - loss: 0.6153 - acc: 0.7610 - val_loss: 0.8168 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 116us/step - loss: 0.6116 - acc: 0.7674 - val_loss: 0.8941 - val_acc: 0.5882\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.6125 - acc: 0.7586 - val_loss: 0.8102 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6134 - acc: 0.7562 - val_loss: 0.8284 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.6127 - acc: 0.7656 - val_loss: 0.8061 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 87us/step - loss: 0.6093 - acc: 0.7695 - val_loss: 0.8490 - val_acc: 0.6176\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 1s 373us/step - loss: 1.9645 - acc: 0.2517 - val_loss: 1.5740 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.6313 - acc: 0.4176 - val_loss: 1.2620 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.4393 - acc: 0.4708 - val_loss: 1.0868 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.3334 - acc: 0.5152 - val_loss: 1.0054 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.2524 - acc: 0.5465 - val_loss: 0.9291 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 1.2210 - acc: 0.5429 - val_loss: 0.9090 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.1666 - acc: 0.5775 - val_loss: 0.8701 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.1191 - acc: 0.5872 - val_loss: 0.8793 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 1.0983 - acc: 0.5973 - val_loss: 0.8476 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.0701 - acc: 0.6076 - val_loss: 0.8952 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.0528 - acc: 0.6198 - val_loss: 0.8590 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.0291 - acc: 0.6143 - val_loss: 0.8623 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.0108 - acc: 0.6271 - val_loss: 0.8573 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.9979 - acc: 0.6398 - val_loss: 0.8542 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.9663 - acc: 0.6429 - val_loss: 0.8624 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.9619 - acc: 0.6532 - val_loss: 0.8474 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.9487 - acc: 0.6547 - val_loss: 0.8400 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.9347 - acc: 0.6590 - val_loss: 0.8554 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.9182 - acc: 0.6587 - val_loss: 0.8436 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.9073 - acc: 0.6653 - val_loss: 0.8564 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.8866 - acc: 0.6784 - val_loss: 0.8525 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8838 - acc: 0.6726 - val_loss: 0.8625 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.8728 - acc: 0.6781 - val_loss: 0.8541 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.8534 - acc: 0.6848 - val_loss: 0.8437 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8404 - acc: 0.6909 - val_loss: 0.8690 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8365 - acc: 0.6860 - val_loss: 0.8740 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.8241 - acc: 0.6918 - val_loss: 0.8665 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.8252 - acc: 0.6933 - val_loss: 0.8629 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.8096 - acc: 0.6994 - val_loss: 0.8601 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.8017 - acc: 0.7046 - val_loss: 0.8723 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7916 - acc: 0.7067 - val_loss: 0.8571 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7871 - acc: 0.7012 - val_loss: 0.8801 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7784 - acc: 0.7082 - val_loss: 0.8808 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7698 - acc: 0.7116 - val_loss: 0.8633 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7681 - acc: 0.7143 - val_loss: 0.8644 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.7692 - acc: 0.7046 - val_loss: 0.8594 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7378 - acc: 0.7143 - val_loss: 0.8518 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7409 - acc: 0.7280 - val_loss: 0.8628 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.7344 - acc: 0.7219 - val_loss: 0.8785 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7160 - acc: 0.7201 - val_loss: 0.8716 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.7216 - acc: 0.7310 - val_loss: 0.8918 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.7082 - acc: 0.7401 - val_loss: 0.8755 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7021 - acc: 0.7410 - val_loss: 0.8880 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.7056 - acc: 0.7404 - val_loss: 0.8674 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.7038 - acc: 0.7362 - val_loss: 0.8628 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6880 - acc: 0.7416 - val_loss: 0.8751 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6937 - acc: 0.7377 - val_loss: 0.8578 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6596 - acc: 0.7471 - val_loss: 0.8989 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6754 - acc: 0.7465 - val_loss: 0.9003 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6598 - acc: 0.7535 - val_loss: 0.8809 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6505 - acc: 0.7568 - val_loss: 0.8499 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6398 - acc: 0.7629 - val_loss: 0.8923 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.6385 - acc: 0.7480 - val_loss: 0.8900 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6403 - acc: 0.7511 - val_loss: 0.8957 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.6320 - acc: 0.7544 - val_loss: 0.9115 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6406 - acc: 0.7605 - val_loss: 0.8816 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6322 - acc: 0.7596 - val_loss: 0.8908 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.6320 - acc: 0.7562 - val_loss: 0.8810 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6150 - acc: 0.7693 - val_loss: 0.8695 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.6090 - acc: 0.7708 - val_loss: 0.8871 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 1s 378us/step - loss: 1.9629 - acc: 0.2626 - val_loss: 1.6112 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 1.6516 - acc: 0.4052 - val_loss: 1.3297 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 1.4387 - acc: 0.4827 - val_loss: 1.1706 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.3270 - acc: 0.5264 - val_loss: 1.0394 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 1.2606 - acc: 0.5419 - val_loss: 0.9947 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.1994 - acc: 0.5629 - val_loss: 0.9247 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.1640 - acc: 0.5748 - val_loss: 0.8269 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.1210 - acc: 0.5821 - val_loss: 0.8581 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.1072 - acc: 0.5930 - val_loss: 0.8213 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.0691 - acc: 0.6033 - val_loss: 0.8546 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.0344 - acc: 0.6143 - val_loss: 0.8093 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.0262 - acc: 0.6185 - val_loss: 0.8453 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.0037 - acc: 0.6295 - val_loss: 0.8469 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.9856 - acc: 0.6353 - val_loss: 0.8247 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.9841 - acc: 0.6459 - val_loss: 0.8305 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.9474 - acc: 0.6520 - val_loss: 0.8295 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9359 - acc: 0.6574 - val_loss: 0.8382 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.9041 - acc: 0.6720 - val_loss: 0.8388 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9114 - acc: 0.6657 - val_loss: 0.8370 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8977 - acc: 0.6766 - val_loss: 0.8203 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8704 - acc: 0.6772 - val_loss: 0.8017 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8571 - acc: 0.6900 - val_loss: 0.7673 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.8590 - acc: 0.6878 - val_loss: 0.7652 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8472 - acc: 0.6939 - val_loss: 0.8141 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8444 - acc: 0.6884 - val_loss: 0.7909 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.8161 - acc: 0.7006 - val_loss: 0.8180 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.8197 - acc: 0.7006 - val_loss: 0.8039 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8052 - acc: 0.7009 - val_loss: 0.8067 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7982 - acc: 0.7100 - val_loss: 0.7869 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.7855 - acc: 0.7076 - val_loss: 0.8132 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.7777 - acc: 0.7116 - val_loss: 0.7889 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.7676 - acc: 0.7140 - val_loss: 0.7903 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7605 - acc: 0.7185 - val_loss: 0.7825 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.7397 - acc: 0.7207 - val_loss: 0.7754 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.7395 - acc: 0.7316 - val_loss: 0.7772 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7172 - acc: 0.7304 - val_loss: 0.7780 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7249 - acc: 0.7243 - val_loss: 0.7678 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 98us/step - loss: 0.7210 - acc: 0.7407 - val_loss: 0.7842 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 89us/step - loss: 0.7107 - acc: 0.7386 - val_loss: 0.7574 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6991 - acc: 0.7410 - val_loss: 0.7712 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6969 - acc: 0.7365 - val_loss: 0.7918 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 107us/step - loss: 0.6851 - acc: 0.7426 - val_loss: 0.7889 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.6947 - acc: 0.7404 - val_loss: 0.7890 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6831 - acc: 0.7347 - val_loss: 0.7638 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6717 - acc: 0.7456 - val_loss: 0.8107 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6497 - acc: 0.7568 - val_loss: 0.8036 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.6572 - acc: 0.7465 - val_loss: 0.8026 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.6585 - acc: 0.7568 - val_loss: 0.7653 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.6546 - acc: 0.7489 - val_loss: 0.7496 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.6328 - acc: 0.7632 - val_loss: 0.7737 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6367 - acc: 0.7562 - val_loss: 0.7972 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6370 - acc: 0.7657 - val_loss: 0.8264 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6169 - acc: 0.7702 - val_loss: 0.8015 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6154 - acc: 0.7793 - val_loss: 0.8026 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6105 - acc: 0.7678 - val_loss: 0.7978 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.5940 - acc: 0.7745 - val_loss: 0.8083 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6101 - acc: 0.7723 - val_loss: 0.7897 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.6015 - acc: 0.7723 - val_loss: 0.8223 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 99us/step - loss: 0.5855 - acc: 0.7754 - val_loss: 0.8057 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 83us/step - loss: 0.6039 - acc: 0.7714 - val_loss: 0.7858 - val_acc: 0.6765\n",
      "Accuracy: 0.6618 ± 0.0248\n",
      "NMI: 0.4458 ± 0.0345\n",
      "Log_loss: 0.9866 ± 0.0873\n",
      "Normalized confusion matrix\n",
      "[[ 0.65861027  0.02416918  0.0060423   0.1858006   0.06797583  0.03021148\n",
      "   0.02719033  0.          0.        ]\n",
      " [ 0.03012048  0.48393574  0.00200803  0.03212851  0.00803213  0.04016064\n",
      "   0.40160643  0.          0.00200803]\n",
      " [ 0.05208333  0.02083333  0.36458333  0.1875      0.04166667  0.\n",
      "   0.33333333  0.          0.        ]\n",
      " [ 0.16910786  0.01997337  0.00932091  0.72569907  0.03195739  0.00798935\n",
      "   0.03462051  0.          0.00133156]\n",
      " [ 0.21348315  0.0411985   0.01872659  0.13108614  0.36329588  0.06367041\n",
      "   0.16853933  0.          0.        ]\n",
      " [ 0.11784512  0.07070707  0.01346801  0.04377104  0.05387205  0.61616162\n",
      "   0.08417508  0.          0.        ]\n",
      " [ 0.01518027  0.10056926  0.02466793  0.0113852   0.01233397  0.00664137\n",
      "   0.82447818  0.          0.00474383]\n",
      " [ 0.0952381   0.33333333  0.          0.04761905  0.          0.\n",
      "   0.19047619  0.04761905  0.28571429]\n",
      " [ 0.02325581  0.04651163  0.          0.04651163  0.02325581  0.\n",
      "   0.06976744  0.          0.79069767]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEgCAYAAADWs+oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a396cd358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the textmodel from ../model/doc2vec/textModel_win=2_no_outside\n",
      "Use test model: textModel_win=2_no_outside\n",
      "begin training\n",
      "\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 2s 618us/step - loss: 1.9703 - acc: 0.2703 - val_loss: 1.5696 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 1.6059 - acc: 0.4093 - val_loss: 1.3645 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 1.4222 - acc: 0.4675 - val_loss: 1.2299 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 1.3299 - acc: 0.5008 - val_loss: 1.1269 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.2693 - acc: 0.5230 - val_loss: 1.0226 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.2122 - acc: 0.5364 - val_loss: 0.9443 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.1778 - acc: 0.5571 - val_loss: 0.8849 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.1499 - acc: 0.5730 - val_loss: 0.8949 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.1149 - acc: 0.5748 - val_loss: 0.8911 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0900 - acc: 0.5931 - val_loss: 0.8537 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0524 - acc: 0.6071 - val_loss: 0.8382 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 1.0457 - acc: 0.6026 - val_loss: 0.8123 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.0227 - acc: 0.6138 - val_loss: 0.8145 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.9975 - acc: 0.6215 - val_loss: 0.8439 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.9930 - acc: 0.6306 - val_loss: 0.8323 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.9519 - acc: 0.6507 - val_loss: 0.8386 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.9393 - acc: 0.6507 - val_loss: 0.8292 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.9241 - acc: 0.6528 - val_loss: 0.8006 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.9144 - acc: 0.6586 - val_loss: 0.8255 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.8975 - acc: 0.6699 - val_loss: 0.8158 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.8886 - acc: 0.6620 - val_loss: 0.7887 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.8758 - acc: 0.6766 - val_loss: 0.8298 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.8646 - acc: 0.6791 - val_loss: 0.8118 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.8417 - acc: 0.6861 - val_loss: 0.8346 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.8274 - acc: 0.6882 - val_loss: 0.8412 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.8239 - acc: 0.7022 - val_loss: 0.8301 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.8220 - acc: 0.6894 - val_loss: 0.8543 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.8156 - acc: 0.6903 - val_loss: 0.8447 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 87us/step - loss: 0.7905 - acc: 0.7086 - val_loss: 0.8493 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7777 - acc: 0.7095 - val_loss: 0.8315 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.7765 - acc: 0.7022 - val_loss: 0.8783 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.7608 - acc: 0.7202 - val_loss: 0.8290 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7580 - acc: 0.7208 - val_loss: 0.8699 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7374 - acc: 0.7230 - val_loss: 0.8955 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.7297 - acc: 0.7263 - val_loss: 0.8635 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7301 - acc: 0.7205 - val_loss: 0.9038 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.7116 - acc: 0.7342 - val_loss: 0.8850 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7076 - acc: 0.7351 - val_loss: 0.8823 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7000 - acc: 0.7364 - val_loss: 0.8788 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7041 - acc: 0.7321 - val_loss: 0.8593 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6746 - acc: 0.7437 - val_loss: 0.9298 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6676 - acc: 0.7507 - val_loss: 0.9059 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 0.6769 - acc: 0.7354 - val_loss: 0.8993 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.6592 - acc: 0.7546 - val_loss: 0.9091 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 70us/step - loss: 0.6492 - acc: 0.7604 - val_loss: 0.9010 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6494 - acc: 0.7571 - val_loss: 0.9373 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 74us/step - loss: 0.6485 - acc: 0.7610 - val_loss: 0.9279 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 73us/step - loss: 0.6330 - acc: 0.7565 - val_loss: 0.9528 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6236 - acc: 0.7598 - val_loss: 0.9564 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 70us/step - loss: 0.6089 - acc: 0.7650 - val_loss: 0.9453 - val_acc: 0.5882\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 75us/step - loss: 0.6201 - acc: 0.7629 - val_loss: 0.9750 - val_acc: 0.5882\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.6038 - acc: 0.7714 - val_loss: 0.9695 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 0.5939 - acc: 0.7732 - val_loss: 1.0041 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.5927 - acc: 0.7720 - val_loss: 0.9954 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.5863 - acc: 0.7799 - val_loss: 0.9608 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.5782 - acc: 0.7781 - val_loss: 1.0004 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 79us/step - loss: 0.5658 - acc: 0.7787 - val_loss: 1.0274 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.5773 - acc: 0.7854 - val_loss: 0.9478 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.5602 - acc: 0.7839 - val_loss: 1.0183 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 84us/step - loss: 0.5623 - acc: 0.7867 - val_loss: 0.9824 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 1s 424us/step - loss: 1.9915 - acc: 0.2665 - val_loss: 1.6001 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 1.6004 - acc: 0.4140 - val_loss: 1.3387 - val_acc: 0.4706\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 1.4328 - acc: 0.4743 - val_loss: 1.2144 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 1.3321 - acc: 0.4962 - val_loss: 1.1214 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 1.2580 - acc: 0.5157 - val_loss: 1.0239 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 1.2240 - acc: 0.5465 - val_loss: 0.9716 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 77us/step - loss: 1.1872 - acc: 0.5571 - val_loss: 0.9765 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.1477 - acc: 0.5705 - val_loss: 0.9564 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 1.1146 - acc: 0.5894 - val_loss: 0.9285 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.0959 - acc: 0.5876 - val_loss: 0.8960 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 1.0672 - acc: 0.6080 - val_loss: 0.8951 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 1.0547 - acc: 0.6104 - val_loss: 0.8991 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.0176 - acc: 0.6296 - val_loss: 0.8377 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 1.0071 - acc: 0.6290 - val_loss: 0.8836 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 80us/step - loss: 0.9734 - acc: 0.6357 - val_loss: 0.8810 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.9615 - acc: 0.6418 - val_loss: 0.8756 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 86us/step - loss: 0.9458 - acc: 0.6628 - val_loss: 0.8603 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.9294 - acc: 0.6576 - val_loss: 0.8463 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.9212 - acc: 0.6628 - val_loss: 0.8538 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.9030 - acc: 0.6741 - val_loss: 0.8925 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.8951 - acc: 0.6707 - val_loss: 0.8811 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.8749 - acc: 0.6780 - val_loss: 0.8680 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.8579 - acc: 0.6869 - val_loss: 0.9010 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.8465 - acc: 0.6866 - val_loss: 0.8444 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8431 - acc: 0.6997 - val_loss: 0.8559 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8140 - acc: 0.7030 - val_loss: 0.8504 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.8127 - acc: 0.7051 - val_loss: 0.8455 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7967 - acc: 0.7030 - val_loss: 0.9057 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.7751 - acc: 0.7173 - val_loss: 0.8857 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 82us/step - loss: 0.7800 - acc: 0.7115 - val_loss: 0.9124 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 76us/step - loss: 0.7710 - acc: 0.7118 - val_loss: 0.8794 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 78us/step - loss: 0.7635 - acc: 0.7198 - val_loss: 0.9043 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 77us/step - loss: 0.7487 - acc: 0.7222 - val_loss: 0.8841 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.7363 - acc: 0.7271 - val_loss: 0.8760 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.7295 - acc: 0.7356 - val_loss: 0.8957 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.7182 - acc: 0.7335 - val_loss: 0.8819 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.7178 - acc: 0.7216 - val_loss: 0.8797 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.7085 - acc: 0.7426 - val_loss: 0.8926 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6989 - acc: 0.7387 - val_loss: 0.9057 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 84us/step - loss: 0.6776 - acc: 0.7463 - val_loss: 0.9002 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6714 - acc: 0.7533 - val_loss: 0.8872 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.6778 - acc: 0.7426 - val_loss: 0.8949 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6564 - acc: 0.7569 - val_loss: 0.9136 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6544 - acc: 0.7609 - val_loss: 0.9156 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.6447 - acc: 0.7645 - val_loss: 0.9882 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.6260 - acc: 0.7688 - val_loss: 0.9464 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.6333 - acc: 0.7722 - val_loss: 0.9188 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6186 - acc: 0.7737 - val_loss: 0.9544 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.6150 - acc: 0.7746 - val_loss: 0.9718 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.6147 - acc: 0.7685 - val_loss: 0.9644 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.6009 - acc: 0.7700 - val_loss: 0.9716 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 81us/step - loss: 0.5879 - acc: 0.7691 - val_loss: 0.9571 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.5983 - acc: 0.7706 - val_loss: 0.9561 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.6033 - acc: 0.7749 - val_loss: 0.9673 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.5846 - acc: 0.7856 - val_loss: 0.9798 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.5799 - acc: 0.7856 - val_loss: 0.9502 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.5688 - acc: 0.7846 - val_loss: 0.9356 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.5461 - acc: 0.7907 - val_loss: 1.0017 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 84us/step - loss: 0.5404 - acc: 0.7968 - val_loss: 0.9732 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.5482 - acc: 0.7947 - val_loss: 0.9866 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 1s 429us/step - loss: 1.9714 - acc: 0.2774 - val_loss: 1.5585 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.5933 - acc: 0.4324 - val_loss: 1.3463 - val_acc: 0.4706\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 1.4020 - acc: 0.4759 - val_loss: 1.2413 - val_acc: 0.4412\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 1.3172 - acc: 0.5140 - val_loss: 1.2001 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 1.2508 - acc: 0.5487 - val_loss: 1.0329 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 1.2106 - acc: 0.5527 - val_loss: 1.1432 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.1719 - acc: 0.5682 - val_loss: 1.0057 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 1.1244 - acc: 0.5941 - val_loss: 1.0028 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 74us/step - loss: 1.1051 - acc: 0.5904 - val_loss: 1.0014 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 1.0695 - acc: 0.6169 - val_loss: 0.9931 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 1.0514 - acc: 0.6157 - val_loss: 1.0306 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 1.0315 - acc: 0.6276 - val_loss: 0.9700 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 1.0032 - acc: 0.6297 - val_loss: 0.9739 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 75us/step - loss: 1.0018 - acc: 0.6337 - val_loss: 0.9450 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 91us/step - loss: 0.9718 - acc: 0.6465 - val_loss: 0.9940 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.9577 - acc: 0.6529 - val_loss: 0.9951 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.9330 - acc: 0.6626 - val_loss: 0.9894 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.9125 - acc: 0.6577 - val_loss: 0.9594 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.9125 - acc: 0.6660 - val_loss: 0.9919 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8810 - acc: 0.6857 - val_loss: 0.9789 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.8727 - acc: 0.6873 - val_loss: 0.9598 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8616 - acc: 0.6909 - val_loss: 0.9377 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8489 - acc: 0.6918 - val_loss: 0.9415 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.8497 - acc: 0.7028 - val_loss: 0.9926 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8293 - acc: 0.6961 - val_loss: 0.9771 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8148 - acc: 0.6988 - val_loss: 0.9873 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8055 - acc: 0.7119 - val_loss: 0.9803 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7930 - acc: 0.7037 - val_loss: 1.0123 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7734 - acc: 0.7195 - val_loss: 0.9675 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7647 - acc: 0.7144 - val_loss: 1.0011 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7555 - acc: 0.7278 - val_loss: 0.9865 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7452 - acc: 0.7250 - val_loss: 0.9736 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7396 - acc: 0.7293 - val_loss: 0.9753 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7452 - acc: 0.7162 - val_loss: 0.9742 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7159 - acc: 0.7302 - val_loss: 1.0081 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7113 - acc: 0.7421 - val_loss: 0.9881 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7072 - acc: 0.7427 - val_loss: 0.9748 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6888 - acc: 0.7403 - val_loss: 1.0099 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6961 - acc: 0.7491 - val_loss: 1.0089 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6784 - acc: 0.7467 - val_loss: 0.9944 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6629 - acc: 0.7488 - val_loss: 0.9928 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.6677 - acc: 0.7463 - val_loss: 1.0225 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6676 - acc: 0.7439 - val_loss: 1.0161 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6395 - acc: 0.7540 - val_loss: 1.0208 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6497 - acc: 0.7655 - val_loss: 1.0156 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6346 - acc: 0.7604 - val_loss: 1.0438 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6340 - acc: 0.7631 - val_loss: 1.0533 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6166 - acc: 0.7686 - val_loss: 1.0364 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6216 - acc: 0.7671 - val_loss: 1.0723 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.6038 - acc: 0.7683 - val_loss: 1.0665 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.5970 - acc: 0.7707 - val_loss: 1.0605 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5943 - acc: 0.7768 - val_loss: 1.0802 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.5819 - acc: 0.7798 - val_loss: 1.1070 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5812 - acc: 0.7722 - val_loss: 1.1304 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.5908 - acc: 0.7713 - val_loss: 1.0668 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5669 - acc: 0.7960 - val_loss: 1.0630 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 62us/step - loss: 0.5759 - acc: 0.7856 - val_loss: 1.1427 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5664 - acc: 0.7832 - val_loss: 1.1126 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5532 - acc: 0.7945 - val_loss: 1.1247 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.5552 - acc: 0.7862 - val_loss: 1.0969 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 1s 427us/step - loss: 1.9817 - acc: 0.2685 - val_loss: 1.5892 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.6648 - acc: 0.4027 - val_loss: 1.3462 - val_acc: 0.5000\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 1.4438 - acc: 0.4642 - val_loss: 1.2044 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 1.3470 - acc: 0.5093 - val_loss: 1.0857 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.2664 - acc: 0.5272 - val_loss: 1.0036 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 1.2214 - acc: 0.5537 - val_loss: 0.9536 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.1910 - acc: 0.5607 - val_loss: 0.9484 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 1.1488 - acc: 0.5921 - val_loss: 0.9198 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 1.1283 - acc: 0.5808 - val_loss: 0.9132 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.0970 - acc: 0.5893 - val_loss: 0.8946 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 1.0714 - acc: 0.6024 - val_loss: 0.8941 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.0476 - acc: 0.6170 - val_loss: 0.8823 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 1.0281 - acc: 0.6164 - val_loss: 0.8994 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 1.0204 - acc: 0.6280 - val_loss: 0.8716 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.9921 - acc: 0.6429 - val_loss: 0.8942 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.9811 - acc: 0.6374 - val_loss: 0.8660 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.9629 - acc: 0.6545 - val_loss: 0.8500 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.9302 - acc: 0.6645 - val_loss: 0.8384 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.9319 - acc: 0.6630 - val_loss: 0.8663 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.9020 - acc: 0.6682 - val_loss: 0.8514 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8968 - acc: 0.6709 - val_loss: 0.8797 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.8715 - acc: 0.6801 - val_loss: 0.8471 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.8677 - acc: 0.6877 - val_loss: 0.8434 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.8532 - acc: 0.6977 - val_loss: 0.8408 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8417 - acc: 0.6910 - val_loss: 0.8580 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8304 - acc: 0.6950 - val_loss: 0.8829 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8182 - acc: 0.7072 - val_loss: 0.8892 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.8050 - acc: 0.7038 - val_loss: 0.8844 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.7998 - acc: 0.7084 - val_loss: 0.8845 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.7998 - acc: 0.7093 - val_loss: 0.8727 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.7833 - acc: 0.7099 - val_loss: 0.8766 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.7630 - acc: 0.7227 - val_loss: 0.8972 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.7656 - acc: 0.7233 - val_loss: 0.9098 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.7507 - acc: 0.7175 - val_loss: 0.9179 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.7408 - acc: 0.7209 - val_loss: 0.9087 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.7338 - acc: 0.7254 - val_loss: 0.8867 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.7166 - acc: 0.7385 - val_loss: 0.9029 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.7148 - acc: 0.7376 - val_loss: 0.9272 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.7106 - acc: 0.7419 - val_loss: 0.9377 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.7089 - acc: 0.7422 - val_loss: 0.9224 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.6905 - acc: 0.7373 - val_loss: 0.9427 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.6779 - acc: 0.7470 - val_loss: 0.9253 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.6798 - acc: 0.7458 - val_loss: 0.9334 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.6633 - acc: 0.7473 - val_loss: 0.9419 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.6603 - acc: 0.7504 - val_loss: 0.9347 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.6455 - acc: 0.7595 - val_loss: 0.9223 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.6387 - acc: 0.7601 - val_loss: 0.9241 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6380 - acc: 0.7562 - val_loss: 0.9261 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.6330 - acc: 0.7604 - val_loss: 0.9397 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6180 - acc: 0.7683 - val_loss: 0.9656 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.6263 - acc: 0.7656 - val_loss: 0.9384 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.6235 - acc: 0.7647 - val_loss: 0.9263 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.5984 - acc: 0.7796 - val_loss: 0.9358 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5956 - acc: 0.7766 - val_loss: 0.9844 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.5984 - acc: 0.7817 - val_loss: 0.9572 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.5878 - acc: 0.7656 - val_loss: 0.9773 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.5732 - acc: 0.7884 - val_loss: 0.9662 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.5800 - acc: 0.7796 - val_loss: 0.9728 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 63us/step - loss: 0.5630 - acc: 0.7906 - val_loss: 0.9997 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5678 - acc: 0.7848 - val_loss: 0.9788 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 1s 444us/step - loss: 1.9660 - acc: 0.2526 - val_loss: 1.6010 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.6716 - acc: 0.4105 - val_loss: 1.3684 - val_acc: 0.5588\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 1.4636 - acc: 0.4580 - val_loss: 1.3253 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 1.3542 - acc: 0.4988 - val_loss: 1.1696 - val_acc: 0.5882\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.2822 - acc: 0.5243 - val_loss: 1.0378 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.2261 - acc: 0.5390 - val_loss: 0.9936 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 1.1673 - acc: 0.5660 - val_loss: 1.0279 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.1451 - acc: 0.5770 - val_loss: 0.9460 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 1.0947 - acc: 0.5980 - val_loss: 0.9118 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.0872 - acc: 0.5886 - val_loss: 0.9052 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.0491 - acc: 0.6068 - val_loss: 0.9294 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.0315 - acc: 0.6123 - val_loss: 0.9047 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 1.0075 - acc: 0.6236 - val_loss: 0.8901 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.9809 - acc: 0.6379 - val_loss: 0.8656 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.9682 - acc: 0.6391 - val_loss: 0.9102 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.9481 - acc: 0.6573 - val_loss: 0.8847 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.9515 - acc: 0.6403 - val_loss: 0.8808 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.9249 - acc: 0.6567 - val_loss: 0.8632 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 0.8973 - acc: 0.6680 - val_loss: 0.8432 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.8999 - acc: 0.6656 - val_loss: 0.8484 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.8745 - acc: 0.6704 - val_loss: 0.8803 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.8643 - acc: 0.6765 - val_loss: 0.8271 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8519 - acc: 0.6911 - val_loss: 0.8176 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8331 - acc: 0.6975 - val_loss: 0.8782 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8195 - acc: 0.7005 - val_loss: 0.8496 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.8218 - acc: 0.6905 - val_loss: 0.8423 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.8121 - acc: 0.6996 - val_loss: 0.8406 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8067 - acc: 0.6963 - val_loss: 0.8366 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.7830 - acc: 0.7100 - val_loss: 0.8361 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7672 - acc: 0.7094 - val_loss: 0.8667 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7629 - acc: 0.7109 - val_loss: 0.8670 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.7522 - acc: 0.7167 - val_loss: 0.8323 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7442 - acc: 0.7222 - val_loss: 0.8381 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.7412 - acc: 0.7225 - val_loss: 0.8752 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7224 - acc: 0.7310 - val_loss: 0.8328 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7155 - acc: 0.7288 - val_loss: 0.8423 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7146 - acc: 0.7395 - val_loss: 0.8323 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7064 - acc: 0.7456 - val_loss: 0.8250 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6913 - acc: 0.7410 - val_loss: 0.8523 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6783 - acc: 0.7471 - val_loss: 0.8360 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6677 - acc: 0.7459 - val_loss: 0.8434 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6630 - acc: 0.7489 - val_loss: 0.8577 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6649 - acc: 0.7538 - val_loss: 0.8694 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6410 - acc: 0.7581 - val_loss: 0.8365 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6425 - acc: 0.7626 - val_loss: 0.8539 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.6383 - acc: 0.7617 - val_loss: 0.8694 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.6289 - acc: 0.7684 - val_loss: 0.8313 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.6375 - acc: 0.7553 - val_loss: 0.8607 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6131 - acc: 0.7705 - val_loss: 0.8380 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 0.6147 - acc: 0.7712 - val_loss: 0.8830 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.5992 - acc: 0.7702 - val_loss: 0.8163 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.5995 - acc: 0.7772 - val_loss: 0.8408 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.5843 - acc: 0.7748 - val_loss: 0.8422 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 89us/step - loss: 0.5850 - acc: 0.7781 - val_loss: 0.8788 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.5783 - acc: 0.7830 - val_loss: 0.8848 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 85us/step - loss: 0.5717 - acc: 0.7842 - val_loss: 0.8287 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 109us/step - loss: 0.5635 - acc: 0.7861 - val_loss: 0.8702 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 86us/step - loss: 0.5625 - acc: 0.7921 - val_loss: 0.8713 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.5543 - acc: 0.7915 - val_loss: 0.8839 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.5532 - acc: 0.7931 - val_loss: 0.9158 - val_acc: 0.8235\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 2s 465us/step - loss: 1.9461 - acc: 0.2949 - val_loss: 1.5676 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.6552 - acc: 0.4060 - val_loss: 1.3402 - val_acc: 0.4706\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.4477 - acc: 0.4641 - val_loss: 1.2079 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.3405 - acc: 0.5091 - val_loss: 1.1481 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 84us/step - loss: 1.2693 - acc: 0.5350 - val_loss: 1.0737 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.2161 - acc: 0.5609 - val_loss: 1.0255 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 91us/step - loss: 1.1782 - acc: 0.5630 - val_loss: 1.0065 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.1378 - acc: 0.5733 - val_loss: 1.0017 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.1152 - acc: 0.5946 - val_loss: 0.9624 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 82us/step - loss: 1.0808 - acc: 0.6007 - val_loss: 0.9508 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.0537 - acc: 0.6208 - val_loss: 0.9611 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0356 - acc: 0.6178 - val_loss: 0.9149 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 1.0156 - acc: 0.6205 - val_loss: 0.9580 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.9955 - acc: 0.6406 - val_loss: 0.9336 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9695 - acc: 0.6424 - val_loss: 0.9462 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.9568 - acc: 0.6491 - val_loss: 0.9237 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.9453 - acc: 0.6528 - val_loss: 0.9377 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 82us/step - loss: 0.9051 - acc: 0.6656 - val_loss: 0.9539 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.9101 - acc: 0.6616 - val_loss: 0.9428 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.8875 - acc: 0.6713 - val_loss: 0.9778 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.8764 - acc: 0.6780 - val_loss: 0.8968 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.8704 - acc: 0.6902 - val_loss: 0.9600 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.8673 - acc: 0.6896 - val_loss: 0.9400 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8341 - acc: 0.6990 - val_loss: 0.9508 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.8255 - acc: 0.7005 - val_loss: 0.9057 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.8176 - acc: 0.6999 - val_loss: 0.9726 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.8025 - acc: 0.6987 - val_loss: 0.9440 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8017 - acc: 0.7097 - val_loss: 0.9889 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7781 - acc: 0.7139 - val_loss: 1.0012 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.7670 - acc: 0.7243 - val_loss: 0.9895 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.7549 - acc: 0.7200 - val_loss: 0.9561 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7441 - acc: 0.7292 - val_loss: 1.0151 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7501 - acc: 0.7270 - val_loss: 0.9190 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.7231 - acc: 0.7325 - val_loss: 1.0183 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7223 - acc: 0.7310 - val_loss: 0.9997 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7113 - acc: 0.7428 - val_loss: 0.9822 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7046 - acc: 0.7340 - val_loss: 1.0520 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.7026 - acc: 0.7428 - val_loss: 0.9918 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6942 - acc: 0.7459 - val_loss: 1.0416 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6752 - acc: 0.7593 - val_loss: 0.9736 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 85us/step - loss: 0.6674 - acc: 0.7535 - val_loss: 1.0230 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 82us/step - loss: 0.6714 - acc: 0.7505 - val_loss: 1.0587 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 89us/step - loss: 0.6532 - acc: 0.7614 - val_loss: 1.0626 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6492 - acc: 0.7532 - val_loss: 1.0521 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6387 - acc: 0.7599 - val_loss: 1.0428 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6246 - acc: 0.7708 - val_loss: 1.0702 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6294 - acc: 0.7620 - val_loss: 1.0660 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6111 - acc: 0.7736 - val_loss: 1.0307 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6204 - acc: 0.7708 - val_loss: 1.0787 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 0.5977 - acc: 0.7772 - val_loss: 1.0249 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.6062 - acc: 0.7736 - val_loss: 1.0445 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6094 - acc: 0.7724 - val_loss: 1.0577 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.5995 - acc: 0.7739 - val_loss: 1.0539 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.5793 - acc: 0.7821 - val_loss: 1.0819 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.5673 - acc: 0.7882 - val_loss: 1.1035 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.5624 - acc: 0.7842 - val_loss: 1.0794 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.5771 - acc: 0.7851 - val_loss: 1.0952 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.5435 - acc: 0.7925 - val_loss: 1.0881 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.5564 - acc: 0.7873 - val_loss: 1.1232 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 88us/step - loss: 0.5484 - acc: 0.7934 - val_loss: 1.1518 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 2s 459us/step - loss: 1.9737 - acc: 0.2647 - val_loss: 1.5755 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 1.6311 - acc: 0.4211 - val_loss: 1.2911 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 1.4373 - acc: 0.4703 - val_loss: 1.2311 - val_acc: 0.5882\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 65us/step - loss: 1.3379 - acc: 0.4962 - val_loss: 1.0518 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 1.2671 - acc: 0.5336 - val_loss: 1.0869 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 1.2134 - acc: 0.5494 - val_loss: 0.9755 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 1.1790 - acc: 0.5701 - val_loss: 0.9633 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 1.1402 - acc: 0.5753 - val_loss: 0.9769 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 1.1129 - acc: 0.5929 - val_loss: 0.9229 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 89us/step - loss: 1.0793 - acc: 0.5929 - val_loss: 0.9646 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 86us/step - loss: 1.0641 - acc: 0.6078 - val_loss: 0.9503 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 1.0459 - acc: 0.6200 - val_loss: 0.9410 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 1.0061 - acc: 0.6206 - val_loss: 0.9330 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.9965 - acc: 0.6374 - val_loss: 0.8969 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 63us/step - loss: 0.9833 - acc: 0.6352 - val_loss: 0.8964 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.9617 - acc: 0.6507 - val_loss: 0.9185 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.9490 - acc: 0.6507 - val_loss: 0.8631 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 98us/step - loss: 0.9333 - acc: 0.6529 - val_loss: 0.8916 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.9038 - acc: 0.6596 - val_loss: 0.9087 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.8778 - acc: 0.6796 - val_loss: 0.8531 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.8775 - acc: 0.6784 - val_loss: 0.9064 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.8820 - acc: 0.6696 - val_loss: 0.8838 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.8778 - acc: 0.6766 - val_loss: 0.8645 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.8421 - acc: 0.6894 - val_loss: 0.9111 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.8345 - acc: 0.6982 - val_loss: 0.8785 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.8186 - acc: 0.6985 - val_loss: 0.9163 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 0.8099 - acc: 0.7046 - val_loss: 0.9019 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.7836 - acc: 0.7049 - val_loss: 0.9232 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 77us/step - loss: 0.7861 - acc: 0.7040 - val_loss: 0.9054 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.7701 - acc: 0.7082 - val_loss: 0.9269 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.7613 - acc: 0.7113 - val_loss: 0.9295 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 0.7546 - acc: 0.7177 - val_loss: 0.9444 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.7514 - acc: 0.7235 - val_loss: 0.9415 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.7300 - acc: 0.7335 - val_loss: 0.9437 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.7287 - acc: 0.7381 - val_loss: 0.9331 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.7229 - acc: 0.7311 - val_loss: 0.9158 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.7157 - acc: 0.7225 - val_loss: 0.9317 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6985 - acc: 0.7444 - val_loss: 0.9259 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6937 - acc: 0.7405 - val_loss: 0.9255 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.6905 - acc: 0.7417 - val_loss: 0.9252 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.6830 - acc: 0.7396 - val_loss: 0.9314 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6541 - acc: 0.7478 - val_loss: 0.9185 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.6635 - acc: 0.7478 - val_loss: 0.9259 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6615 - acc: 0.7478 - val_loss: 0.9619 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6397 - acc: 0.7551 - val_loss: 0.9155 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.6503 - acc: 0.7454 - val_loss: 0.9596 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6420 - acc: 0.7557 - val_loss: 0.9678 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6196 - acc: 0.7639 - val_loss: 0.9823 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6218 - acc: 0.7587 - val_loss: 0.9888 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.6114 - acc: 0.7758 - val_loss: 0.9410 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6055 - acc: 0.7664 - val_loss: 0.9697 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6080 - acc: 0.7657 - val_loss: 0.9862 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.5930 - acc: 0.7755 - val_loss: 0.9896 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.5906 - acc: 0.7727 - val_loss: 1.0038 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.5770 - acc: 0.7752 - val_loss: 0.9650 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.5802 - acc: 0.7764 - val_loss: 0.9771 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.5742 - acc: 0.7837 - val_loss: 0.9555 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.5609 - acc: 0.7867 - val_loss: 0.9892 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.5550 - acc: 0.7873 - val_loss: 0.9953 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.5419 - acc: 0.7968 - val_loss: 0.9773 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 2s 484us/step - loss: 1.9712 - acc: 0.2870 - val_loss: 1.5162 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 1.5921 - acc: 0.4351 - val_loss: 1.2742 - val_acc: 0.5000\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 1.4112 - acc: 0.4722 - val_loss: 1.1558 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 1.3090 - acc: 0.5172 - val_loss: 1.0841 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 1.2592 - acc: 0.5357 - val_loss: 1.0323 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 1.2017 - acc: 0.5597 - val_loss: 0.9331 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 1.1729 - acc: 0.5731 - val_loss: 0.9376 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.1215 - acc: 0.5856 - val_loss: 0.9385 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 1.1156 - acc: 0.5886 - val_loss: 0.8576 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 1.0788 - acc: 0.6060 - val_loss: 0.8957 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 1.0382 - acc: 0.6251 - val_loss: 0.8637 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 1.0219 - acc: 0.6315 - val_loss: 0.8777 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 80us/step - loss: 0.9970 - acc: 0.6342 - val_loss: 0.8965 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.9865 - acc: 0.6382 - val_loss: 0.9081 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.9651 - acc: 0.6467 - val_loss: 0.9224 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.9403 - acc: 0.6573 - val_loss: 0.8867 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.9297 - acc: 0.6607 - val_loss: 0.9496 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.9079 - acc: 0.6728 - val_loss: 0.9502 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.8841 - acc: 0.6768 - val_loss: 0.9301 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.8821 - acc: 0.6756 - val_loss: 0.9284 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.8706 - acc: 0.6859 - val_loss: 0.9425 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8574 - acc: 0.6926 - val_loss: 0.9401 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.8416 - acc: 0.6972 - val_loss: 0.9685 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.8278 - acc: 0.7023 - val_loss: 0.9842 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8077 - acc: 0.7112 - val_loss: 0.9409 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8075 - acc: 0.7029 - val_loss: 0.9702 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7974 - acc: 0.7112 - val_loss: 0.9607 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7687 - acc: 0.7151 - val_loss: 0.9301 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7613 - acc: 0.7230 - val_loss: 0.9364 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7602 - acc: 0.7175 - val_loss: 0.9672 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7599 - acc: 0.7221 - val_loss: 0.9709 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7242 - acc: 0.7315 - val_loss: 1.0165 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7231 - acc: 0.7306 - val_loss: 1.0120 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7121 - acc: 0.7367 - val_loss: 1.0062 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 63us/step - loss: 0.7131 - acc: 0.7373 - val_loss: 0.9758 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7072 - acc: 0.7382 - val_loss: 1.0318 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6970 - acc: 0.7403 - val_loss: 1.0221 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.6837 - acc: 0.7476 - val_loss: 0.9983 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.6738 - acc: 0.7501 - val_loss: 1.0144 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.6731 - acc: 0.7492 - val_loss: 1.0290 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6647 - acc: 0.7510 - val_loss: 1.0611 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6664 - acc: 0.7501 - val_loss: 1.0284 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.6513 - acc: 0.7574 - val_loss: 1.0494 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.6441 - acc: 0.7549 - val_loss: 1.0299 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 76us/step - loss: 0.6408 - acc: 0.7583 - val_loss: 1.0856 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.6357 - acc: 0.7665 - val_loss: 1.0839 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.6200 - acc: 0.7656 - val_loss: 1.0678 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6219 - acc: 0.7698 - val_loss: 1.1026 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.6040 - acc: 0.7683 - val_loss: 1.0750 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6006 - acc: 0.7787 - val_loss: 1.0769 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.5960 - acc: 0.7729 - val_loss: 1.1184 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.5897 - acc: 0.7704 - val_loss: 1.1352 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.5868 - acc: 0.7726 - val_loss: 1.1153 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.5676 - acc: 0.7838 - val_loss: 1.1148 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.5721 - acc: 0.7881 - val_loss: 1.1659 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.5646 - acc: 0.7802 - val_loss: 1.1479 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.5542 - acc: 0.7948 - val_loss: 1.1090 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.5490 - acc: 0.7954 - val_loss: 1.1651 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.5402 - acc: 0.7972 - val_loss: 1.1680 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.5453 - acc: 0.7957 - val_loss: 1.1637 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 528us/step - loss: 1.9526 - acc: 0.2942 - val_loss: 1.6023 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 81us/step - loss: 1.6276 - acc: 0.4119 - val_loss: 1.3431 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.4480 - acc: 0.4574 - val_loss: 1.1833 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.3365 - acc: 0.5100 - val_loss: 1.0765 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 1.2862 - acc: 0.5173 - val_loss: 1.0108 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.2380 - acc: 0.5353 - val_loss: 0.9711 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.1860 - acc: 0.5596 - val_loss: 0.9165 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 1.1570 - acc: 0.5720 - val_loss: 0.9284 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.1238 - acc: 0.5836 - val_loss: 0.8921 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.0938 - acc: 0.5915 - val_loss: 0.8912 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.0737 - acc: 0.6003 - val_loss: 0.8873 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 1.0456 - acc: 0.6161 - val_loss: 0.8507 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 1.0207 - acc: 0.6149 - val_loss: 0.8910 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 82us/step - loss: 1.0120 - acc: 0.6213 - val_loss: 0.8351 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 84us/step - loss: 0.9832 - acc: 0.6401 - val_loss: 0.8562 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.9648 - acc: 0.6486 - val_loss: 0.8414 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.9423 - acc: 0.6587 - val_loss: 0.8377 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.9317 - acc: 0.6553 - val_loss: 0.8508 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9270 - acc: 0.6574 - val_loss: 0.8791 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 86us/step - loss: 0.9056 - acc: 0.6587 - val_loss: 0.8346 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.8932 - acc: 0.6763 - val_loss: 0.8485 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.8759 - acc: 0.6793 - val_loss: 0.8236 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8634 - acc: 0.6805 - val_loss: 0.8576 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.8438 - acc: 0.6979 - val_loss: 0.8286 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8366 - acc: 0.6979 - val_loss: 0.8734 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8189 - acc: 0.7030 - val_loss: 0.8488 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.8122 - acc: 0.6942 - val_loss: 0.8387 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7949 - acc: 0.7036 - val_loss: 0.8859 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7919 - acc: 0.7103 - val_loss: 0.8714 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.7849 - acc: 0.7000 - val_loss: 0.8623 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7722 - acc: 0.7152 - val_loss: 0.8567 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7552 - acc: 0.7188 - val_loss: 0.9094 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.7519 - acc: 0.7204 - val_loss: 0.8744 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7520 - acc: 0.7185 - val_loss: 0.8451 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.7271 - acc: 0.7307 - val_loss: 0.9289 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7161 - acc: 0.7340 - val_loss: 0.8936 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7146 - acc: 0.7356 - val_loss: 0.8709 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6965 - acc: 0.7380 - val_loss: 0.9179 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6917 - acc: 0.7447 - val_loss: 0.9180 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.6815 - acc: 0.7444 - val_loss: 0.9254 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6794 - acc: 0.7416 - val_loss: 0.8908 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.6655 - acc: 0.7471 - val_loss: 0.9250 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6719 - acc: 0.7529 - val_loss: 0.9381 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.6560 - acc: 0.7565 - val_loss: 0.9383 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6393 - acc: 0.7632 - val_loss: 0.9409 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6368 - acc: 0.7578 - val_loss: 0.9770 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.6317 - acc: 0.7559 - val_loss: 0.9234 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6225 - acc: 0.7638 - val_loss: 0.9706 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.6091 - acc: 0.7733 - val_loss: 0.9671 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5973 - acc: 0.7745 - val_loss: 0.9847 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.5995 - acc: 0.7687 - val_loss: 0.9540 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.5998 - acc: 0.7672 - val_loss: 0.9690 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5942 - acc: 0.7711 - val_loss: 0.9668 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.5858 - acc: 0.7793 - val_loss: 0.9546 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5968 - acc: 0.7812 - val_loss: 0.9560 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.5806 - acc: 0.7711 - val_loss: 0.9713 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.5653 - acc: 0.7860 - val_loss: 0.9607 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5639 - acc: 0.7918 - val_loss: 0.9563 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5650 - acc: 0.7830 - val_loss: 0.9254 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.5550 - acc: 0.7897 - val_loss: 0.9192 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 495us/step - loss: 1.9682 - acc: 0.2708 - val_loss: 1.6178 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.6325 - acc: 0.4131 - val_loss: 1.3655 - val_acc: 0.4118\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 1.4465 - acc: 0.4526 - val_loss: 1.2503 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.3384 - acc: 0.4979 - val_loss: 1.1282 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.2705 - acc: 0.5252 - val_loss: 1.0586 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.2227 - acc: 0.5480 - val_loss: 1.0408 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.1839 - acc: 0.5614 - val_loss: 0.9521 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.1504 - acc: 0.5720 - val_loss: 0.9416 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 1.1128 - acc: 0.5906 - val_loss: 0.9648 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.0837 - acc: 0.5973 - val_loss: 0.8941 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.0622 - acc: 0.6094 - val_loss: 0.8865 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.0382 - acc: 0.6128 - val_loss: 0.9231 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.0277 - acc: 0.6243 - val_loss: 0.8588 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9973 - acc: 0.6322 - val_loss: 0.9071 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.9808 - acc: 0.6371 - val_loss: 0.8907 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9495 - acc: 0.6477 - val_loss: 0.8929 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9375 - acc: 0.6602 - val_loss: 0.8533 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.9244 - acc: 0.6638 - val_loss: 0.8422 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.9089 - acc: 0.6653 - val_loss: 0.8637 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.8971 - acc: 0.6733 - val_loss: 0.8325 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8760 - acc: 0.6815 - val_loss: 0.8319 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.8574 - acc: 0.6872 - val_loss: 0.8596 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.8415 - acc: 0.6860 - val_loss: 0.8802 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8405 - acc: 0.6936 - val_loss: 0.8569 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.8272 - acc: 0.6976 - val_loss: 0.8608 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.8138 - acc: 0.7049 - val_loss: 0.8669 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7939 - acc: 0.7140 - val_loss: 0.8530 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.7835 - acc: 0.7149 - val_loss: 0.8563 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7935 - acc: 0.7082 - val_loss: 0.8367 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7642 - acc: 0.7167 - val_loss: 0.8694 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.7596 - acc: 0.7198 - val_loss: 0.8434 - val_acc: 0.7647\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7534 - acc: 0.7195 - val_loss: 0.8877 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7427 - acc: 0.7295 - val_loss: 0.8718 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7207 - acc: 0.7371 - val_loss: 0.8867 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7254 - acc: 0.7356 - val_loss: 0.8760 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.7181 - acc: 0.7337 - val_loss: 0.8700 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.7023 - acc: 0.7395 - val_loss: 0.8826 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6859 - acc: 0.7407 - val_loss: 0.9260 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.6838 - acc: 0.7462 - val_loss: 0.8921 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6778 - acc: 0.7453 - val_loss: 0.9222 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.6710 - acc: 0.7468 - val_loss: 0.8858 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6577 - acc: 0.7480 - val_loss: 0.8900 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6473 - acc: 0.7520 - val_loss: 0.9394 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.6513 - acc: 0.7565 - val_loss: 0.9333 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.6399 - acc: 0.7629 - val_loss: 0.9097 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6383 - acc: 0.7641 - val_loss: 0.9113 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6283 - acc: 0.7711 - val_loss: 0.9337 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6088 - acc: 0.7672 - val_loss: 0.8992 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.5992 - acc: 0.7805 - val_loss: 0.9461 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6127 - acc: 0.7708 - val_loss: 0.9364 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6035 - acc: 0.7641 - val_loss: 0.9256 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5943 - acc: 0.7793 - val_loss: 0.9693 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.5669 - acc: 0.7845 - val_loss: 0.9706 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5836 - acc: 0.7739 - val_loss: 0.9971 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5753 - acc: 0.7903 - val_loss: 0.9634 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5604 - acc: 0.7863 - val_loss: 0.9802 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.5516 - acc: 0.7930 - val_loss: 1.0173 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 63us/step - loss: 0.5605 - acc: 0.7799 - val_loss: 0.9718 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5394 - acc: 0.7988 - val_loss: 0.9694 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.5455 - acc: 0.7869 - val_loss: 0.9560 - val_acc: 0.6765\n",
      "Accuracy: 0.6598 ± 0.0170\n",
      "NMI: 0.4432 ± 0.0301\n",
      "Log_loss: 1.0273 ± 0.0726\n",
      "Normalized confusion matrix\n",
      "[[ 0.66918429  0.01963746  0.00302115  0.17371601  0.06797583  0.03927492\n",
      "   0.02719033  0.          0.        ]\n",
      " [ 0.04016064  0.48594378  0.0060241   0.02208835  0.01405622  0.02409639\n",
      "   0.40361446  0.          0.00401606]\n",
      " [ 0.0625      0.          0.35416667  0.19791667  0.04166667  0.01041667\n",
      "   0.33333333  0.          0.        ]\n",
      " [ 0.1930759   0.01731025  0.01065246  0.69906791  0.04394141  0.00665779\n",
      "   0.02663116  0.          0.00266312]\n",
      " [ 0.22097378  0.02996255  0.01498127  0.11985019  0.3670412   0.07490637\n",
      "   0.17228464  0.          0.        ]\n",
      " [ 0.14141414  0.06397306  0.01010101  0.04377104  0.05050505  0.60942761\n",
      "   0.08080808  0.          0.        ]\n",
      " [ 0.01043643  0.09487666  0.02182163  0.02087287  0.01518027  0.0056926\n",
      "   0.82827324  0.00094877  0.00189753]\n",
      " [ 0.04761905  0.28571429  0.          0.          0.          0.\n",
      "   0.33333333  0.14285714  0.19047619]\n",
      " [ 0.04651163  0.          0.          0.02325581  0.02325581  0.\n",
      "   0.09302326  0.          0.81395349]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEgCAYAAADWs+oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUJTC/wOvrNwYHLFinwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4be6f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the textmodel from ../model/doc2vec/docEmbeddings_30_load_all.d2v\n",
      "Use test model: docEmbeddings_30_load_all.d2v\n",
      "begin training\n",
      "\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 2s 497us/step - loss: 2.0019 - acc: 0.2450 - val_loss: 1.4991 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.6203 - acc: 0.4127 - val_loss: 1.2994 - val_acc: 0.5882\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.4277 - acc: 0.4700 - val_loss: 1.2229 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.3361 - acc: 0.5136 - val_loss: 1.0941 - val_acc: 0.5588\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.2702 - acc: 0.5398 - val_loss: 1.0163 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.2242 - acc: 0.5447 - val_loss: 0.9487 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.1794 - acc: 0.5547 - val_loss: 0.9382 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.1324 - acc: 0.5736 - val_loss: 0.8805 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 1.1222 - acc: 0.5733 - val_loss: 0.8764 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 1.0928 - acc: 0.5922 - val_loss: 0.8580 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.0546 - acc: 0.6102 - val_loss: 0.8452 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 72us/step - loss: 1.0375 - acc: 0.6154 - val_loss: 0.8542 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 1.0136 - acc: 0.6236 - val_loss: 0.8224 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.9968 - acc: 0.6269 - val_loss: 0.8451 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.9784 - acc: 0.6358 - val_loss: 0.8214 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.9621 - acc: 0.6358 - val_loss: 0.8357 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.9441 - acc: 0.6495 - val_loss: 0.8292 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.9305 - acc: 0.6580 - val_loss: 0.8249 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.9207 - acc: 0.6593 - val_loss: 0.8135 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.9022 - acc: 0.6629 - val_loss: 0.8309 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.8767 - acc: 0.6657 - val_loss: 0.8102 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.8892 - acc: 0.6684 - val_loss: 0.8214 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.8664 - acc: 0.6733 - val_loss: 0.8136 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.8559 - acc: 0.6781 - val_loss: 0.7983 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.8507 - acc: 0.6745 - val_loss: 0.8094 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.8388 - acc: 0.6778 - val_loss: 0.8213 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.8266 - acc: 0.6900 - val_loss: 0.7995 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.8311 - acc: 0.6821 - val_loss: 0.8343 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.8033 - acc: 0.7013 - val_loss: 0.8116 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7929 - acc: 0.7056 - val_loss: 0.8151 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7962 - acc: 0.6964 - val_loss: 0.8108 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7845 - acc: 0.7041 - val_loss: 0.8125 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7640 - acc: 0.7086 - val_loss: 0.8096 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.7584 - acc: 0.7132 - val_loss: 0.8327 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7562 - acc: 0.7141 - val_loss: 0.8075 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7487 - acc: 0.7220 - val_loss: 0.8219 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7383 - acc: 0.7135 - val_loss: 0.8190 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7245 - acc: 0.7190 - val_loss: 0.8268 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7262 - acc: 0.7272 - val_loss: 0.8172 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7186 - acc: 0.7236 - val_loss: 0.8106 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7017 - acc: 0.7324 - val_loss: 0.8297 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.7059 - acc: 0.7269 - val_loss: 0.8139 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.7173 - acc: 0.7162 - val_loss: 0.8264 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6941 - acc: 0.7330 - val_loss: 0.8393 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6892 - acc: 0.7382 - val_loss: 0.8251 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6707 - acc: 0.7452 - val_loss: 0.8766 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6760 - acc: 0.7400 - val_loss: 0.8474 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6705 - acc: 0.7425 - val_loss: 0.8784 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6589 - acc: 0.7537 - val_loss: 0.8734 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6557 - acc: 0.7473 - val_loss: 0.8754 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6476 - acc: 0.7418 - val_loss: 0.8532 - val_acc: 0.7353\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6377 - acc: 0.7543 - val_loss: 0.8672 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6300 - acc: 0.7571 - val_loss: 0.8653 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6225 - acc: 0.7665 - val_loss: 0.8739 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6304 - acc: 0.7598 - val_loss: 0.8622 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6037 - acc: 0.7693 - val_loss: 0.8659 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.6109 - acc: 0.7614 - val_loss: 0.9102 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 62us/step - loss: 0.6225 - acc: 0.7510 - val_loss: 0.8979 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 63us/step - loss: 0.5996 - acc: 0.7638 - val_loss: 0.8930 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 64us/step - loss: 0.6013 - acc: 0.7620 - val_loss: 0.8815 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 2s 504us/step - loss: 1.9265 - acc: 0.2684 - val_loss: 1.6165 - val_acc: 0.4412\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.6760 - acc: 0.3932 - val_loss: 1.3976 - val_acc: 0.5000\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 1.4693 - acc: 0.4673 - val_loss: 1.2592 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.3621 - acc: 0.5032 - val_loss: 1.1810 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.2862 - acc: 0.5318 - val_loss: 1.0996 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.2239 - acc: 0.5535 - val_loss: 1.0409 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 1.1906 - acc: 0.5614 - val_loss: 0.9810 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 1.1542 - acc: 0.5730 - val_loss: 0.9209 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.1242 - acc: 0.5882 - val_loss: 0.9404 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 1.1131 - acc: 0.5940 - val_loss: 0.8936 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.0659 - acc: 0.6165 - val_loss: 0.9127 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.0506 - acc: 0.6122 - val_loss: 0.8432 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 1.0444 - acc: 0.6183 - val_loss: 0.8783 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 1.0009 - acc: 0.6333 - val_loss: 0.8579 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 1.0037 - acc: 0.6339 - val_loss: 0.8814 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.9692 - acc: 0.6403 - val_loss: 0.8432 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.9566 - acc: 0.6497 - val_loss: 0.8930 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.9504 - acc: 0.6476 - val_loss: 0.8485 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.9335 - acc: 0.6506 - val_loss: 0.8374 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.9280 - acc: 0.6543 - val_loss: 0.8430 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 65us/step - loss: 0.9003 - acc: 0.6732 - val_loss: 0.8661 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 63us/step - loss: 0.8957 - acc: 0.6707 - val_loss: 0.8387 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.8777 - acc: 0.6707 - val_loss: 0.8554 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8735 - acc: 0.6735 - val_loss: 0.8447 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8568 - acc: 0.6860 - val_loss: 0.8546 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.8351 - acc: 0.7000 - val_loss: 0.8485 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.8423 - acc: 0.6878 - val_loss: 0.8756 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.8270 - acc: 0.6945 - val_loss: 0.8729 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8269 - acc: 0.6960 - val_loss: 0.8516 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7956 - acc: 0.7033 - val_loss: 0.8347 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.8017 - acc: 0.7003 - val_loss: 0.8589 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7760 - acc: 0.7128 - val_loss: 0.8361 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.7835 - acc: 0.6997 - val_loss: 0.8459 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7545 - acc: 0.7164 - val_loss: 0.8322 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.7613 - acc: 0.7143 - val_loss: 0.9077 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.7382 - acc: 0.7262 - val_loss: 0.8528 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.7472 - acc: 0.7216 - val_loss: 0.8584 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.7341 - acc: 0.7256 - val_loss: 0.8707 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.7383 - acc: 0.7189 - val_loss: 0.8452 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 64us/step - loss: 0.7285 - acc: 0.7207 - val_loss: 0.8380 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.6953 - acc: 0.7359 - val_loss: 0.8691 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.7036 - acc: 0.7362 - val_loss: 0.8552 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.6941 - acc: 0.7432 - val_loss: 0.8920 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.6937 - acc: 0.7417 - val_loss: 0.8810 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.6757 - acc: 0.7514 - val_loss: 0.9048 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.6794 - acc: 0.7341 - val_loss: 0.8980 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6587 - acc: 0.7457 - val_loss: 0.9153 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.6673 - acc: 0.7475 - val_loss: 0.8859 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.6444 - acc: 0.7588 - val_loss: 0.9357 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.6500 - acc: 0.7508 - val_loss: 0.8965 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6346 - acc: 0.7627 - val_loss: 0.8836 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.6352 - acc: 0.7524 - val_loss: 0.9253 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.6355 - acc: 0.7530 - val_loss: 0.9168 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6244 - acc: 0.7600 - val_loss: 0.9013 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.6194 - acc: 0.7664 - val_loss: 0.9107 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.6241 - acc: 0.7630 - val_loss: 0.9178 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.5962 - acc: 0.7630 - val_loss: 0.8899 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.5937 - acc: 0.7792 - val_loss: 0.9207 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.6060 - acc: 0.7703 - val_loss: 0.9421 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6017 - acc: 0.7673 - val_loss: 0.9233 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 2s 515us/step - loss: 1.8914 - acc: 0.3045 - val_loss: 1.5021 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 1.6472 - acc: 0.4035 - val_loss: 1.2903 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.4672 - acc: 0.4595 - val_loss: 1.1500 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.3482 - acc: 0.5131 - val_loss: 1.0309 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.2839 - acc: 0.5274 - val_loss: 0.9986 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.2416 - acc: 0.5417 - val_loss: 0.9648 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.2006 - acc: 0.5493 - val_loss: 0.9492 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 1.1615 - acc: 0.5639 - val_loss: 0.9039 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.1331 - acc: 0.5865 - val_loss: 0.9912 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 1.1294 - acc: 0.5776 - val_loss: 0.9136 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.0959 - acc: 0.5871 - val_loss: 0.8831 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.0706 - acc: 0.6084 - val_loss: 0.8753 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.0339 - acc: 0.6175 - val_loss: 0.8828 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 67us/step - loss: 1.0231 - acc: 0.6282 - val_loss: 0.8576 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 1.0169 - acc: 0.6227 - val_loss: 0.8540 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.9872 - acc: 0.6349 - val_loss: 0.8922 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.9811 - acc: 0.6386 - val_loss: 0.8514 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.9640 - acc: 0.6452 - val_loss: 0.8838 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.9452 - acc: 0.6516 - val_loss: 0.8570 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.9415 - acc: 0.6477 - val_loss: 0.8810 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.9164 - acc: 0.6526 - val_loss: 0.8526 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.9155 - acc: 0.6590 - val_loss: 0.8917 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8988 - acc: 0.6647 - val_loss: 0.8742 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8747 - acc: 0.6705 - val_loss: 0.8822 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.8703 - acc: 0.6751 - val_loss: 0.8987 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8617 - acc: 0.6781 - val_loss: 0.9038 - val_acc: 0.5000\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8537 - acc: 0.6790 - val_loss: 0.8808 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8442 - acc: 0.6891 - val_loss: 0.8861 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8192 - acc: 0.6958 - val_loss: 0.9360 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.8202 - acc: 0.6964 - val_loss: 0.9071 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.8293 - acc: 0.6931 - val_loss: 0.9283 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7964 - acc: 0.7022 - val_loss: 0.9146 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7963 - acc: 0.6964 - val_loss: 0.9552 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7865 - acc: 0.7046 - val_loss: 0.9543 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7723 - acc: 0.7086 - val_loss: 0.9771 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7638 - acc: 0.7046 - val_loss: 0.9546 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7519 - acc: 0.7183 - val_loss: 0.9662 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7480 - acc: 0.7211 - val_loss: 0.9609 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7471 - acc: 0.7177 - val_loss: 0.9808 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7307 - acc: 0.7141 - val_loss: 0.9447 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 0.7258 - acc: 0.7232 - val_loss: 0.9911 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.7314 - acc: 0.7189 - val_loss: 1.0028 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7066 - acc: 0.7296 - val_loss: 0.9967 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 65us/step - loss: 0.7053 - acc: 0.7378 - val_loss: 1.0097 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6841 - acc: 0.7363 - val_loss: 1.0141 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.7029 - acc: 0.7357 - val_loss: 1.0057 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6731 - acc: 0.7479 - val_loss: 1.0017 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6784 - acc: 0.7439 - val_loss: 1.0326 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6552 - acc: 0.7427 - val_loss: 1.0113 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6602 - acc: 0.7533 - val_loss: 1.0443 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6494 - acc: 0.7558 - val_loss: 1.0235 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6520 - acc: 0.7494 - val_loss: 1.0322 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6525 - acc: 0.7491 - val_loss: 1.0538 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6406 - acc: 0.7497 - val_loss: 1.0819 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6300 - acc: 0.7613 - val_loss: 1.0722 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6398 - acc: 0.7671 - val_loss: 1.0716 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6277 - acc: 0.7607 - val_loss: 1.0924 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6196 - acc: 0.7637 - val_loss: 1.0942 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 64us/step - loss: 0.6230 - acc: 0.7604 - val_loss: 1.1108 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 63us/step - loss: 0.6086 - acc: 0.7719 - val_loss: 1.0963 - val_acc: 0.5588\n",
      "begin training\n",
      "\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 2s 545us/step - loss: 1.8791 - acc: 0.3026 - val_loss: 1.5279 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 1.6566 - acc: 0.4030 - val_loss: 1.3006 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 74us/step - loss: 1.4705 - acc: 0.4642 - val_loss: 1.1842 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 1.3551 - acc: 0.5139 - val_loss: 1.1114 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 1.2851 - acc: 0.5330 - val_loss: 1.0248 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 1.2308 - acc: 0.5495 - val_loss: 0.9713 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 80us/step - loss: 1.1922 - acc: 0.5647 - val_loss: 0.9728 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 1.1506 - acc: 0.5726 - val_loss: 0.9193 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 80us/step - loss: 1.1211 - acc: 0.5890 - val_loss: 0.9047 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 75us/step - loss: 1.0826 - acc: 0.6030 - val_loss: 0.8740 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 1.0770 - acc: 0.6110 - val_loss: 0.8685 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 1.0522 - acc: 0.6131 - val_loss: 0.8732 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0384 - acc: 0.6219 - val_loss: 0.8474 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 77us/step - loss: 1.0101 - acc: 0.6277 - val_loss: 0.8231 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.9905 - acc: 0.6292 - val_loss: 0.8179 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.9925 - acc: 0.6280 - val_loss: 0.8231 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.9677 - acc: 0.6411 - val_loss: 0.8122 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.9511 - acc: 0.6478 - val_loss: 0.7853 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.9315 - acc: 0.6521 - val_loss: 0.8028 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.9122 - acc: 0.6591 - val_loss: 0.8047 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 77us/step - loss: 0.9007 - acc: 0.6615 - val_loss: 0.7731 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.8924 - acc: 0.6642 - val_loss: 0.7728 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.8741 - acc: 0.6831 - val_loss: 0.7722 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.8661 - acc: 0.6712 - val_loss: 0.7823 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.8571 - acc: 0.6779 - val_loss: 0.7626 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 75us/step - loss: 0.8378 - acc: 0.6871 - val_loss: 0.8038 - val_acc: 0.5882\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.8318 - acc: 0.6913 - val_loss: 0.8276 - val_acc: 0.6176\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 0s 74us/step - loss: 0.8177 - acc: 0.6904 - val_loss: 0.7707 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.8156 - acc: 0.6868 - val_loss: 0.7728 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.8069 - acc: 0.6953 - val_loss: 0.7841 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.7861 - acc: 0.7023 - val_loss: 0.7852 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.7972 - acc: 0.6998 - val_loss: 0.7927 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.7858 - acc: 0.7065 - val_loss: 0.7955 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.7609 - acc: 0.7108 - val_loss: 0.7957 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.7500 - acc: 0.7145 - val_loss: 0.7861 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.7367 - acc: 0.7215 - val_loss: 0.7804 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.7345 - acc: 0.7260 - val_loss: 0.7673 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.7386 - acc: 0.7090 - val_loss: 0.7963 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.7206 - acc: 0.7260 - val_loss: 0.8079 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.7320 - acc: 0.7193 - val_loss: 0.7987 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 76us/step - loss: 0.7071 - acc: 0.7291 - val_loss: 0.7980 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 66us/step - loss: 0.7072 - acc: 0.7282 - val_loss: 0.7715 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.6929 - acc: 0.7321 - val_loss: 0.7811 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.6913 - acc: 0.7333 - val_loss: 0.8188 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.6832 - acc: 0.7476 - val_loss: 0.7983 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.6802 - acc: 0.7425 - val_loss: 0.8119 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.6642 - acc: 0.7549 - val_loss: 0.8301 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.6564 - acc: 0.7510 - val_loss: 0.8286 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 77us/step - loss: 0.6663 - acc: 0.7528 - val_loss: 0.8068 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 74us/step - loss: 0.6449 - acc: 0.7583 - val_loss: 0.8486 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.6451 - acc: 0.7489 - val_loss: 0.8254 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 73us/step - loss: 0.6283 - acc: 0.7583 - val_loss: 0.8528 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 0.6488 - acc: 0.7467 - val_loss: 0.8322 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 78us/step - loss: 0.6249 - acc: 0.7549 - val_loss: 0.8491 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6196 - acc: 0.7619 - val_loss: 0.8365 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.6170 - acc: 0.7623 - val_loss: 0.8437 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 76us/step - loss: 0.6022 - acc: 0.7744 - val_loss: 0.8443 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 65us/step - loss: 0.5972 - acc: 0.7720 - val_loss: 0.8511 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 64us/step - loss: 0.5918 - acc: 0.7714 - val_loss: 0.8792 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.6030 - acc: 0.7668 - val_loss: 0.9011 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 2s 518us/step - loss: 1.9165 - acc: 0.2858 - val_loss: 1.5661 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.6302 - acc: 0.4184 - val_loss: 1.3086 - val_acc: 0.5882\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.4532 - acc: 0.4747 - val_loss: 1.1852 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.3414 - acc: 0.5128 - val_loss: 1.0380 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.2662 - acc: 0.5310 - val_loss: 0.9494 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.2322 - acc: 0.5478 - val_loss: 0.9670 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.1736 - acc: 0.5706 - val_loss: 0.8680 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.1400 - acc: 0.5782 - val_loss: 0.8847 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.1214 - acc: 0.5901 - val_loss: 0.8553 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.0760 - acc: 0.6062 - val_loss: 0.8299 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0712 - acc: 0.6026 - val_loss: 0.8302 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.0508 - acc: 0.6068 - val_loss: 0.7946 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0269 - acc: 0.6199 - val_loss: 0.7772 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.0068 - acc: 0.6272 - val_loss: 0.7829 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9764 - acc: 0.6369 - val_loss: 0.7767 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9749 - acc: 0.6403 - val_loss: 0.7517 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9604 - acc: 0.6446 - val_loss: 0.7811 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9295 - acc: 0.6485 - val_loss: 0.8038 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9294 - acc: 0.6622 - val_loss: 0.7740 - val_acc: 0.7353\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9133 - acc: 0.6625 - val_loss: 0.7579 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9060 - acc: 0.6646 - val_loss: 0.7606 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8902 - acc: 0.6765 - val_loss: 0.7968 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8830 - acc: 0.6692 - val_loss: 0.7925 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8683 - acc: 0.6713 - val_loss: 0.7600 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8536 - acc: 0.6808 - val_loss: 0.7470 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8360 - acc: 0.6905 - val_loss: 0.7785 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8416 - acc: 0.6832 - val_loss: 0.8063 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8254 - acc: 0.6948 - val_loss: 0.7669 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8074 - acc: 0.6929 - val_loss: 0.7735 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.8002 - acc: 0.6966 - val_loss: 0.7800 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7981 - acc: 0.7057 - val_loss: 0.8002 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7927 - acc: 0.6948 - val_loss: 0.7993 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7952 - acc: 0.7015 - val_loss: 0.7606 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7578 - acc: 0.7164 - val_loss: 0.8018 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7696 - acc: 0.7042 - val_loss: 0.7706 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7587 - acc: 0.7091 - val_loss: 0.8051 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.7331 - acc: 0.7173 - val_loss: 0.8250 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7402 - acc: 0.7197 - val_loss: 0.8030 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7288 - acc: 0.7197 - val_loss: 0.8188 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7258 - acc: 0.7228 - val_loss: 0.8122 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7035 - acc: 0.7334 - val_loss: 0.8426 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.7030 - acc: 0.7298 - val_loss: 0.8260 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6891 - acc: 0.7416 - val_loss: 0.8474 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6964 - acc: 0.7334 - val_loss: 0.8344 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6863 - acc: 0.7404 - val_loss: 0.8679 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6747 - acc: 0.7477 - val_loss: 0.8519 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6625 - acc: 0.7349 - val_loss: 0.8306 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6629 - acc: 0.7459 - val_loss: 0.8306 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6488 - acc: 0.7486 - val_loss: 0.8358 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6555 - acc: 0.7389 - val_loss: 0.8197 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6598 - acc: 0.7453 - val_loss: 0.8355 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6526 - acc: 0.7416 - val_loss: 0.8254 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6491 - acc: 0.7553 - val_loss: 0.8789 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6285 - acc: 0.7565 - val_loss: 0.8348 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6353 - acc: 0.7587 - val_loss: 0.8498 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6133 - acc: 0.7572 - val_loss: 0.8467 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6032 - acc: 0.7675 - val_loss: 0.8576 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6143 - acc: 0.7638 - val_loss: 0.8809 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.5972 - acc: 0.7684 - val_loss: 0.8943 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6132 - acc: 0.7648 - val_loss: 0.8931 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 2s 534us/step - loss: 1.9272 - acc: 0.2806 - val_loss: 1.5367 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.6413 - acc: 0.4072 - val_loss: 1.3450 - val_acc: 0.5294\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.4768 - acc: 0.4577 - val_loss: 1.2643 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.3761 - acc: 0.5043 - val_loss: 1.1644 - val_acc: 0.5882\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.2958 - acc: 0.5265 - val_loss: 1.0645 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.2495 - acc: 0.5481 - val_loss: 1.0032 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.2015 - acc: 0.5630 - val_loss: 0.9795 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 1.1765 - acc: 0.5736 - val_loss: 0.9786 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.1382 - acc: 0.5879 - val_loss: 1.0120 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.1048 - acc: 0.5910 - val_loss: 0.9631 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.0881 - acc: 0.5971 - val_loss: 0.9598 - val_acc: 0.6176\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0717 - acc: 0.6068 - val_loss: 0.9595 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0494 - acc: 0.6166 - val_loss: 0.9289 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 1.0394 - acc: 0.6263 - val_loss: 0.8889 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 1.0191 - acc: 0.6248 - val_loss: 0.9109 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9913 - acc: 0.6360 - val_loss: 0.9031 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9784 - acc: 0.6382 - val_loss: 0.9255 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9817 - acc: 0.6421 - val_loss: 0.8948 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.9537 - acc: 0.6497 - val_loss: 0.9117 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9363 - acc: 0.6567 - val_loss: 0.8929 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.9199 - acc: 0.6582 - val_loss: 0.9108 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8946 - acc: 0.6783 - val_loss: 0.9055 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8823 - acc: 0.6780 - val_loss: 0.9512 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8893 - acc: 0.6768 - val_loss: 0.9149 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8825 - acc: 0.6762 - val_loss: 0.9572 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8707 - acc: 0.6744 - val_loss: 0.8868 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 63us/step - loss: 0.8333 - acc: 0.6911 - val_loss: 0.9021 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8394 - acc: 0.6926 - val_loss: 0.9315 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8223 - acc: 0.6957 - val_loss: 0.9212 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.8252 - acc: 0.6926 - val_loss: 0.9166 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.8016 - acc: 0.7002 - val_loss: 0.9173 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7991 - acc: 0.7072 - val_loss: 0.9183 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7903 - acc: 0.7109 - val_loss: 0.9233 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7673 - acc: 0.7088 - val_loss: 0.9555 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7582 - acc: 0.7133 - val_loss: 0.9303 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7560 - acc: 0.7197 - val_loss: 0.9083 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7482 - acc: 0.7267 - val_loss: 0.9256 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7505 - acc: 0.7340 - val_loss: 0.9471 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7407 - acc: 0.7316 - val_loss: 0.8957 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 0.7299 - acc: 0.7304 - val_loss: 0.9022 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.7111 - acc: 0.7355 - val_loss: 0.9593 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.7179 - acc: 0.7358 - val_loss: 0.9412 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7073 - acc: 0.7380 - val_loss: 0.8927 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.7012 - acc: 0.7374 - val_loss: 0.9009 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.7028 - acc: 0.7392 - val_loss: 0.9152 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.6816 - acc: 0.7438 - val_loss: 0.9053 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6835 - acc: 0.7413 - val_loss: 0.9095 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6745 - acc: 0.7456 - val_loss: 0.9379 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 64us/step - loss: 0.6583 - acc: 0.7523 - val_loss: 0.9277 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 83us/step - loss: 0.6647 - acc: 0.7502 - val_loss: 0.9483 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6651 - acc: 0.7529 - val_loss: 0.9170 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6513 - acc: 0.7559 - val_loss: 0.9715 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.6459 - acc: 0.7523 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6356 - acc: 0.7556 - val_loss: 0.8925 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 65us/step - loss: 0.6280 - acc: 0.7699 - val_loss: 0.9651 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 86us/step - loss: 0.6170 - acc: 0.7687 - val_loss: 0.9858 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 66us/step - loss: 0.6112 - acc: 0.7654 - val_loss: 0.9498 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6016 - acc: 0.7797 - val_loss: 1.0220 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6037 - acc: 0.7669 - val_loss: 0.9461 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6046 - acc: 0.7687 - val_loss: 0.9304 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 2s 560us/step - loss: 1.9233 - acc: 0.2884 - val_loss: 1.5271 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 1.6549 - acc: 0.4031 - val_loss: 1.2545 - val_acc: 0.5882\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 1.4600 - acc: 0.4627 - val_loss: 1.1827 - val_acc: 0.6176\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 67us/step - loss: 1.3560 - acc: 0.5068 - val_loss: 1.0608 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 1.2819 - acc: 0.5376 - val_loss: 0.9739 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 1.2155 - acc: 0.5643 - val_loss: 0.9102 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 1.1707 - acc: 0.5729 - val_loss: 0.8713 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 1.1386 - acc: 0.5902 - val_loss: 0.8737 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 1.1004 - acc: 0.5896 - val_loss: 0.8583 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 1.0770 - acc: 0.6091 - val_loss: 0.8476 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 1.0573 - acc: 0.6124 - val_loss: 0.8478 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.0261 - acc: 0.6212 - val_loss: 0.8378 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.0225 - acc: 0.6288 - val_loss: 0.8131 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.9874 - acc: 0.6374 - val_loss: 0.8198 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.9639 - acc: 0.6419 - val_loss: 0.7982 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.9476 - acc: 0.6574 - val_loss: 0.8247 - val_acc: 0.5882\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.9440 - acc: 0.6544 - val_loss: 0.8320 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.9245 - acc: 0.6510 - val_loss: 0.7588 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.9175 - acc: 0.6717 - val_loss: 0.8008 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.9045 - acc: 0.6653 - val_loss: 0.7929 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.8835 - acc: 0.6702 - val_loss: 0.8035 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.8558 - acc: 0.6757 - val_loss: 0.7368 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.8612 - acc: 0.6857 - val_loss: 0.7741 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.8539 - acc: 0.6848 - val_loss: 0.7719 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.8328 - acc: 0.6906 - val_loss: 0.7733 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.8217 - acc: 0.6863 - val_loss: 0.7643 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.8073 - acc: 0.6979 - val_loss: 0.7475 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.8026 - acc: 0.7040 - val_loss: 0.7579 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7904 - acc: 0.7046 - val_loss: 0.7461 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7929 - acc: 0.7089 - val_loss: 0.7592 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7662 - acc: 0.7128 - val_loss: 0.7402 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7567 - acc: 0.7046 - val_loss: 0.7681 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.7625 - acc: 0.7049 - val_loss: 0.7594 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.7341 - acc: 0.7256 - val_loss: 0.7568 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 66us/step - loss: 0.7293 - acc: 0.7192 - val_loss: 0.7188 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.7313 - acc: 0.7250 - val_loss: 0.7363 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7170 - acc: 0.7308 - val_loss: 0.7240 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7241 - acc: 0.7308 - val_loss: 0.7309 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6945 - acc: 0.7344 - val_loss: 0.7358 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.7109 - acc: 0.7271 - val_loss: 0.7429 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.6896 - acc: 0.7350 - val_loss: 0.7396 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6844 - acc: 0.7435 - val_loss: 0.7209 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.6862 - acc: 0.7375 - val_loss: 0.7417 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 0.6555 - acc: 0.7527 - val_loss: 0.7108 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6669 - acc: 0.7487 - val_loss: 0.7265 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6445 - acc: 0.7560 - val_loss: 0.7660 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 0.6492 - acc: 0.7463 - val_loss: 0.7587 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.6392 - acc: 0.7493 - val_loss: 0.7764 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6475 - acc: 0.7569 - val_loss: 0.7535 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 0.6424 - acc: 0.7548 - val_loss: 0.7993 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.6327 - acc: 0.7493 - val_loss: 0.7469 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6151 - acc: 0.7584 - val_loss: 0.7931 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.6144 - acc: 0.7630 - val_loss: 0.7529 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6141 - acc: 0.7654 - val_loss: 0.7953 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.6001 - acc: 0.7624 - val_loss: 0.7683 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.5971 - acc: 0.7648 - val_loss: 0.7611 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.5956 - acc: 0.7651 - val_loss: 0.8141 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.5831 - acc: 0.7651 - val_loss: 0.7743 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 64us/step - loss: 0.5925 - acc: 0.7740 - val_loss: 0.7607 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 65us/step - loss: 0.5763 - acc: 0.7749 - val_loss: 0.7485 - val_acc: 0.7353\n",
      "begin training\n",
      "\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 3s 862us/step - loss: 1.8824 - acc: 0.2971 - val_loss: 1.4661 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 1.6224 - acc: 0.4141 - val_loss: 1.2639 - val_acc: 0.5000\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.4583 - acc: 0.4637 - val_loss: 1.1362 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.3700 - acc: 0.5002 - val_loss: 1.0428 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 1.2998 - acc: 0.5263 - val_loss: 0.9207 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.2367 - acc: 0.5531 - val_loss: 0.8963 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.1822 - acc: 0.5616 - val_loss: 0.8514 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 1.1520 - acc: 0.5819 - val_loss: 0.8383 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.1261 - acc: 0.5892 - val_loss: 0.7987 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 1.0933 - acc: 0.5917 - val_loss: 0.7958 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.0747 - acc: 0.5898 - val_loss: 0.7633 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 1.0451 - acc: 0.6130 - val_loss: 0.7856 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 1.0464 - acc: 0.6102 - val_loss: 0.7780 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.0080 - acc: 0.6242 - val_loss: 0.8065 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 1.0019 - acc: 0.6236 - val_loss: 0.7988 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.9821 - acc: 0.6294 - val_loss: 0.7686 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.9718 - acc: 0.6446 - val_loss: 0.7817 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.9396 - acc: 0.6564 - val_loss: 0.7857 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.9192 - acc: 0.6555 - val_loss: 0.7660 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.9183 - acc: 0.6619 - val_loss: 0.7736 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 0.9008 - acc: 0.6732 - val_loss: 0.7684 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.8952 - acc: 0.6695 - val_loss: 0.7871 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.8945 - acc: 0.6646 - val_loss: 0.7844 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.8755 - acc: 0.6728 - val_loss: 0.7756 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.8547 - acc: 0.6832 - val_loss: 0.8109 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.8503 - acc: 0.6829 - val_loss: 0.7923 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.8261 - acc: 0.6917 - val_loss: 0.8113 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.8153 - acc: 0.7033 - val_loss: 0.7906 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8243 - acc: 0.6865 - val_loss: 0.7804 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8031 - acc: 0.6935 - val_loss: 0.7865 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.8036 - acc: 0.6941 - val_loss: 0.8094 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7846 - acc: 0.7017 - val_loss: 0.8153 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7760 - acc: 0.6963 - val_loss: 0.8015 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7638 - acc: 0.7154 - val_loss: 0.7848 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.7605 - acc: 0.7163 - val_loss: 0.7885 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7536 - acc: 0.7224 - val_loss: 0.8326 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7355 - acc: 0.7254 - val_loss: 0.8116 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7385 - acc: 0.7160 - val_loss: 0.8234 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7376 - acc: 0.7185 - val_loss: 0.7928 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7218 - acc: 0.7236 - val_loss: 0.8465 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.7160 - acc: 0.7276 - val_loss: 0.8521 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.7001 - acc: 0.7294 - val_loss: 0.8387 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6888 - acc: 0.7428 - val_loss: 0.8072 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 0.6852 - acc: 0.7437 - val_loss: 0.8276 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6833 - acc: 0.7422 - val_loss: 0.8171 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.6847 - acc: 0.7376 - val_loss: 0.8743 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6712 - acc: 0.7470 - val_loss: 0.8947 - val_acc: 0.6176\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6556 - acc: 0.7458 - val_loss: 0.8466 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6476 - acc: 0.7555 - val_loss: 0.8393 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6458 - acc: 0.7516 - val_loss: 0.8468 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6491 - acc: 0.7528 - val_loss: 0.8374 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6444 - acc: 0.7522 - val_loss: 0.8810 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6290 - acc: 0.7555 - val_loss: 0.8257 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6226 - acc: 0.7483 - val_loss: 0.8163 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6204 - acc: 0.7601 - val_loss: 0.9007 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6099 - acc: 0.7598 - val_loss: 0.8567 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 64us/step - loss: 0.6124 - acc: 0.7592 - val_loss: 0.8838 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 66us/step - loss: 0.5961 - acc: 0.7735 - val_loss: 0.9109 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.6024 - acc: 0.7671 - val_loss: 0.9084 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 65us/step - loss: 0.5949 - acc: 0.7638 - val_loss: 0.8555 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 575us/step - loss: 1.9020 - acc: 0.2821 - val_loss: 1.5390 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.6285 - acc: 0.4128 - val_loss: 1.3407 - val_acc: 0.4412\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.4695 - acc: 0.4584 - val_loss: 1.1970 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.3698 - acc: 0.4967 - val_loss: 1.1144 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.3008 - acc: 0.5368 - val_loss: 1.0775 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.2529 - acc: 0.5334 - val_loss: 0.9967 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.1996 - acc: 0.5650 - val_loss: 0.9439 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.1598 - acc: 0.5763 - val_loss: 0.9325 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.1207 - acc: 0.5888 - val_loss: 0.8805 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.1075 - acc: 0.5964 - val_loss: 0.8945 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.0706 - acc: 0.6094 - val_loss: 0.8626 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 1.0581 - acc: 0.6012 - val_loss: 0.8503 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 1.0259 - acc: 0.6109 - val_loss: 0.8730 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 1.0111 - acc: 0.6249 - val_loss: 0.8485 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9863 - acc: 0.6459 - val_loss: 0.8593 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9810 - acc: 0.6389 - val_loss: 0.8367 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.9681 - acc: 0.6447 - val_loss: 0.8417 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.9506 - acc: 0.6459 - val_loss: 0.8363 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.9307 - acc: 0.6644 - val_loss: 0.8477 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.9210 - acc: 0.6584 - val_loss: 0.8435 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.9011 - acc: 0.6736 - val_loss: 0.8151 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.8878 - acc: 0.6736 - val_loss: 0.8358 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8930 - acc: 0.6696 - val_loss: 0.8329 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.8662 - acc: 0.6766 - val_loss: 0.8107 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.8569 - acc: 0.6836 - val_loss: 0.8045 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.8603 - acc: 0.6809 - val_loss: 0.8226 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.8437 - acc: 0.6790 - val_loss: 0.8086 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 85us/step - loss: 0.8377 - acc: 0.6933 - val_loss: 0.8352 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.8405 - acc: 0.6851 - val_loss: 0.8195 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.8152 - acc: 0.6891 - val_loss: 0.8261 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.8005 - acc: 0.7030 - val_loss: 0.8111 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8104 - acc: 0.6927 - val_loss: 0.8290 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7945 - acc: 0.7027 - val_loss: 0.8212 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.7793 - acc: 0.7085 - val_loss: 0.8089 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.7861 - acc: 0.7143 - val_loss: 0.8129 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7587 - acc: 0.7137 - val_loss: 0.8256 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7565 - acc: 0.7179 - val_loss: 0.8101 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.7454 - acc: 0.7201 - val_loss: 0.8099 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7445 - acc: 0.7176 - val_loss: 0.8473 - val_acc: 0.6176\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7344 - acc: 0.7213 - val_loss: 0.8582 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.7322 - acc: 0.7191 - val_loss: 0.8686 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7134 - acc: 0.7286 - val_loss: 0.8622 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.7065 - acc: 0.7334 - val_loss: 0.8782 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.7212 - acc: 0.7304 - val_loss: 0.8732 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.7041 - acc: 0.7316 - val_loss: 0.8483 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 98us/step - loss: 0.6962 - acc: 0.7386 - val_loss: 0.8497 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6814 - acc: 0.7359 - val_loss: 0.8413 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 81us/step - loss: 0.6721 - acc: 0.7389 - val_loss: 0.8583 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6704 - acc: 0.7456 - val_loss: 0.8944 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6657 - acc: 0.7447 - val_loss: 0.8578 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6532 - acc: 0.7556 - val_loss: 0.8658 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6588 - acc: 0.7550 - val_loss: 0.8848 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6554 - acc: 0.7483 - val_loss: 0.8662 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6410 - acc: 0.7538 - val_loss: 0.8792 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6214 - acc: 0.7629 - val_loss: 0.8790 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6386 - acc: 0.7562 - val_loss: 0.8581 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6296 - acc: 0.7602 - val_loss: 0.8746 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.6264 - acc: 0.7653 - val_loss: 0.8739 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6168 - acc: 0.7611 - val_loss: 0.9073 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6170 - acc: 0.7666 - val_loss: 0.8671 - val_acc: 0.6471\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 630us/step - loss: 1.9770 - acc: 0.2578 - val_loss: 1.6412 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 1.7048 - acc: 0.3827 - val_loss: 1.4183 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.4913 - acc: 0.4556 - val_loss: 1.2467 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.3867 - acc: 0.4964 - val_loss: 1.1370 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 1.3100 - acc: 0.5301 - val_loss: 1.0526 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.2458 - acc: 0.5429 - val_loss: 1.0626 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.1977 - acc: 0.5626 - val_loss: 0.9749 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 1.1701 - acc: 0.5547 - val_loss: 0.9411 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.1280 - acc: 0.5827 - val_loss: 0.9043 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.1149 - acc: 0.5997 - val_loss: 0.8713 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.0675 - acc: 0.6125 - val_loss: 0.8910 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.0584 - acc: 0.6100 - val_loss: 0.8626 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 1.0389 - acc: 0.6152 - val_loss: 0.8557 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.0047 - acc: 0.6319 - val_loss: 0.8650 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9891 - acc: 0.6413 - val_loss: 0.8835 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.9825 - acc: 0.6386 - val_loss: 0.8280 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.9632 - acc: 0.6502 - val_loss: 0.8447 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9519 - acc: 0.6547 - val_loss: 0.8557 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.9218 - acc: 0.6547 - val_loss: 0.8141 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9122 - acc: 0.6635 - val_loss: 0.8183 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.9022 - acc: 0.6629 - val_loss: 0.8182 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.8942 - acc: 0.6526 - val_loss: 0.8458 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.8847 - acc: 0.6660 - val_loss: 0.8254 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.8747 - acc: 0.6760 - val_loss: 0.8169 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.8497 - acc: 0.6766 - val_loss: 0.8487 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.8464 - acc: 0.6888 - val_loss: 0.8673 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.8293 - acc: 0.6894 - val_loss: 0.8370 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.8365 - acc: 0.6860 - val_loss: 0.8653 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.8104 - acc: 0.6909 - val_loss: 0.8384 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.8038 - acc: 0.6936 - val_loss: 0.8481 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.8043 - acc: 0.6939 - val_loss: 0.8846 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7822 - acc: 0.7085 - val_loss: 0.8894 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.7774 - acc: 0.7070 - val_loss: 0.8631 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.7734 - acc: 0.7073 - val_loss: 0.8506 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7657 - acc: 0.7125 - val_loss: 0.8491 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.7481 - acc: 0.7152 - val_loss: 0.8874 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7456 - acc: 0.7155 - val_loss: 0.8352 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7372 - acc: 0.7210 - val_loss: 0.8667 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7213 - acc: 0.7264 - val_loss: 0.8574 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.7201 - acc: 0.7328 - val_loss: 0.8460 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7078 - acc: 0.7334 - val_loss: 0.8594 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6974 - acc: 0.7404 - val_loss: 0.8953 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.7019 - acc: 0.7225 - val_loss: 0.8703 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6977 - acc: 0.7289 - val_loss: 0.8263 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 64us/step - loss: 0.6771 - acc: 0.7462 - val_loss: 0.8957 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6554 - acc: 0.7568 - val_loss: 0.8717 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6633 - acc: 0.7456 - val_loss: 0.8555 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.6681 - acc: 0.7441 - val_loss: 0.8670 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6608 - acc: 0.7465 - val_loss: 0.8476 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6550 - acc: 0.7426 - val_loss: 0.8447 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6322 - acc: 0.7587 - val_loss: 0.8926 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 65us/step - loss: 0.6325 - acc: 0.7568 - val_loss: 0.9186 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 83us/step - loss: 0.6355 - acc: 0.7526 - val_loss: 0.8926 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 66us/step - loss: 0.6223 - acc: 0.7623 - val_loss: 0.8855 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6120 - acc: 0.7647 - val_loss: 0.8904 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6116 - acc: 0.7617 - val_loss: 0.9289 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.6201 - acc: 0.7611 - val_loss: 0.8968 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6086 - acc: 0.7644 - val_loss: 0.8732 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6014 - acc: 0.7669 - val_loss: 0.8778 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 67us/step - loss: 0.5923 - acc: 0.7726 - val_loss: 0.8668 - val_acc: 0.6765\n",
      "Accuracy: 0.6506 ± 0.0134\n",
      "NMI: 0.4409 ± 0.0170\n",
      "Log_loss: 0.9993 ± 0.0841\n",
      "Normalized confusion matrix\n",
      "[[ 0.68277946  0.01359517  0.0060423   0.18429003  0.06495468  0.02870091\n",
      "   0.01963746  0.          0.        ]\n",
      " [ 0.03413655  0.47389558  0.00401606  0.03012048  0.01405622  0.02409639\n",
      "   0.41767068  0.          0.00200803]\n",
      " [ 0.0625      0.03125     0.34375     0.20833333  0.03125     0.\n",
      "   0.32291667  0.          0.        ]\n",
      " [ 0.19573901  0.01864181  0.01464714  0.69906791  0.03728362  0.00532623\n",
      "   0.02796272  0.          0.00133156]\n",
      " [ 0.25842697  0.04494382  0.01498127  0.11610487  0.35205993  0.06367041\n",
      "   0.14981273  0.          0.        ]\n",
      " [ 0.14478114  0.04040404  0.01010101  0.04040404  0.06060606  0.60942761\n",
      "   0.09427609  0.          0.        ]\n",
      " [ 0.0113852   0.12523719  0.02087287  0.01802657  0.0142315   0.0085389\n",
      "   0.79981025  0.00094877  0.00094877]\n",
      " [ 0.0952381   0.33333333  0.          0.04761905  0.          0.\n",
      "   0.14285714  0.14285714  0.23809524]\n",
      " [ 0.09302326  0.04651163  0.          0.04651163  0.          0.\n",
      "   0.04651163  0.          0.76744186]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEgCAYAAADWs+oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSIVCoTCDCpAKhUJhBhUgFQqFwgz/ByKkWYQ9wTMVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a4c3b9630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the textmodel from ../model/doc2vec/textModel_win=30_no_outside\n",
      "Use test model: textModel_win=30_no_outside\n",
      "begin training\n",
      "\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 2s 601us/step - loss: 1.9213 - acc: 0.2636 - val_loss: 1.5893 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.7712 - acc: 0.3475 - val_loss: 1.4498 - val_acc: 0.4706\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.6736 - acc: 0.3926 - val_loss: 1.3281 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.5727 - acc: 0.4258 - val_loss: 1.2587 - val_acc: 0.5294\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 73us/step - loss: 1.5051 - acc: 0.4489 - val_loss: 1.2035 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 1.4586 - acc: 0.4633 - val_loss: 1.1746 - val_acc: 0.5294\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.4054 - acc: 0.4864 - val_loss: 1.1119 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 72us/step - loss: 1.3686 - acc: 0.4855 - val_loss: 1.1032 - val_acc: 0.5588\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.3293 - acc: 0.5105 - val_loss: 1.0765 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.3000 - acc: 0.5282 - val_loss: 1.0382 - val_acc: 0.5294\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.2683 - acc: 0.5404 - val_loss: 1.0009 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.2410 - acc: 0.5541 - val_loss: 0.9909 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.2200 - acc: 0.5575 - val_loss: 0.9473 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.1861 - acc: 0.5709 - val_loss: 0.9291 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.1542 - acc: 0.5837 - val_loss: 0.8757 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.1309 - acc: 0.5840 - val_loss: 0.8614 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 1.1060 - acc: 0.5949 - val_loss: 0.8419 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 74us/step - loss: 1.0928 - acc: 0.5995 - val_loss: 0.8218 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 1.0705 - acc: 0.6175 - val_loss: 0.7916 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 1.0339 - acc: 0.6285 - val_loss: 0.8068 - val_acc: 0.6176\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 72us/step - loss: 1.0264 - acc: 0.6373 - val_loss: 0.7939 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.9982 - acc: 0.6279 - val_loss: 0.7682 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.9910 - acc: 0.6385 - val_loss: 0.7637 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.9617 - acc: 0.6507 - val_loss: 0.7288 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 75us/step - loss: 0.9630 - acc: 0.6522 - val_loss: 0.7159 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.9305 - acc: 0.6602 - val_loss: 0.6951 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 0.9232 - acc: 0.6562 - val_loss: 0.7014 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 71us/step - loss: 0.8945 - acc: 0.6806 - val_loss: 0.6931 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.8865 - acc: 0.6836 - val_loss: 0.7140 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.8651 - acc: 0.6788 - val_loss: 0.6832 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.8476 - acc: 0.6916 - val_loss: 0.6675 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.8390 - acc: 0.6961 - val_loss: 0.6780 - val_acc: 0.6765\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.8152 - acc: 0.7031 - val_loss: 0.6745 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.8122 - acc: 0.7025 - val_loss: 0.6761 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7935 - acc: 0.7120 - val_loss: 0.6876 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 65us/step - loss: 0.7788 - acc: 0.7181 - val_loss: 0.6917 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7794 - acc: 0.7245 - val_loss: 0.6886 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7582 - acc: 0.7257 - val_loss: 0.6842 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.7560 - acc: 0.7245 - val_loss: 0.6855 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.7370 - acc: 0.7370 - val_loss: 0.7012 - val_acc: 0.6471\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.7227 - acc: 0.7397 - val_loss: 0.6770 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6983 - acc: 0.7458 - val_loss: 0.6975 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.7003 - acc: 0.7455 - val_loss: 0.6937 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.7027 - acc: 0.7467 - val_loss: 0.6823 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6634 - acc: 0.7647 - val_loss: 0.6703 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6618 - acc: 0.7601 - val_loss: 0.6895 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6649 - acc: 0.7626 - val_loss: 0.6905 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.6407 - acc: 0.7617 - val_loss: 0.6771 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6391 - acc: 0.7723 - val_loss: 0.6741 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6251 - acc: 0.7745 - val_loss: 0.7018 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6320 - acc: 0.7644 - val_loss: 0.6857 - val_acc: 0.7353\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.6072 - acc: 0.7699 - val_loss: 0.6985 - val_acc: 0.7941\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 69us/step - loss: 0.6175 - acc: 0.7681 - val_loss: 0.7308 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.5821 - acc: 0.7857 - val_loss: 0.7130 - val_acc: 0.7059\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 68us/step - loss: 0.5837 - acc: 0.7830 - val_loss: 0.7155 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.5808 - acc: 0.7839 - val_loss: 0.7054 - val_acc: 0.7353\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 67us/step - loss: 0.5656 - acc: 0.7906 - val_loss: 0.7179 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.5624 - acc: 0.7934 - val_loss: 0.7013 - val_acc: 0.7647\n",
      "Train on 3281 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3281/3281 [==============================] - 0s 73us/step - loss: 0.5610 - acc: 0.7958 - val_loss: 0.7038 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3281/3281 [==============================] - 0s 66us/step - loss: 0.5484 - acc: 0.8046 - val_loss: 0.7117 - val_acc: 0.7647\n",
      "begin training\n",
      "\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 2s 607us/step - loss: 1.9847 - acc: 0.2473 - val_loss: 1.6623 - val_acc: 0.3235\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.7798 - acc: 0.3238 - val_loss: 1.5109 - val_acc: 0.4118\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.6755 - acc: 0.3801 - val_loss: 1.3474 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.5638 - acc: 0.4286 - val_loss: 1.3103 - val_acc: 0.4706\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.4827 - acc: 0.4487 - val_loss: 1.2303 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.4285 - acc: 0.4733 - val_loss: 1.1718 - val_acc: 0.5294\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.4034 - acc: 0.4852 - val_loss: 1.1365 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.3451 - acc: 0.5056 - val_loss: 1.0998 - val_acc: 0.6176\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.3079 - acc: 0.5172 - val_loss: 1.0572 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.2776 - acc: 0.5306 - val_loss: 1.0038 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 1.2497 - acc: 0.5422 - val_loss: 0.9542 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 1.2176 - acc: 0.5553 - val_loss: 0.9458 - val_acc: 0.6471\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.1871 - acc: 0.5623 - val_loss: 0.9103 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 1.1569 - acc: 0.5809 - val_loss: 0.8940 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 1.1350 - acc: 0.5897 - val_loss: 0.8665 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.1235 - acc: 0.5964 - val_loss: 0.8379 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 1.0962 - acc: 0.6046 - val_loss: 0.8720 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 1.0813 - acc: 0.6043 - val_loss: 0.8635 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 1.0492 - acc: 0.6226 - val_loss: 0.8186 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 1.0222 - acc: 0.6351 - val_loss: 0.7863 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 1.0055 - acc: 0.6378 - val_loss: 0.7922 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.9924 - acc: 0.6430 - val_loss: 0.7589 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.9625 - acc: 0.6659 - val_loss: 0.8366 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.9642 - acc: 0.6521 - val_loss: 0.7626 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.9318 - acc: 0.6689 - val_loss: 0.7487 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.9283 - acc: 0.6588 - val_loss: 0.7568 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 81us/step - loss: 0.8975 - acc: 0.6793 - val_loss: 0.7359 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.8928 - acc: 0.6777 - val_loss: 0.7297 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8887 - acc: 0.6844 - val_loss: 0.7135 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8515 - acc: 0.6963 - val_loss: 0.7157 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.8477 - acc: 0.7015 - val_loss: 0.6916 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 78us/step - loss: 0.8293 - acc: 0.7036 - val_loss: 0.7102 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.8215 - acc: 0.7097 - val_loss: 0.7401 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.8050 - acc: 0.7143 - val_loss: 0.7065 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 73us/step - loss: 0.7918 - acc: 0.7091 - val_loss: 0.6686 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 72us/step - loss: 0.7770 - acc: 0.7201 - val_loss: 0.6804 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.7775 - acc: 0.7189 - val_loss: 0.6708 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.7447 - acc: 0.7393 - val_loss: 0.6931 - val_acc: 0.6765\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.7411 - acc: 0.7371 - val_loss: 0.6935 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 76us/step - loss: 0.7274 - acc: 0.7387 - val_loss: 0.7075 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.7227 - acc: 0.7390 - val_loss: 0.7407 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.6948 - acc: 0.7536 - val_loss: 0.6944 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.7152 - acc: 0.7368 - val_loss: 0.7714 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3283/3283 [==============================] - 0s 69us/step - loss: 0.6927 - acc: 0.7478 - val_loss: 0.7194 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 74us/step - loss: 0.6880 - acc: 0.7554 - val_loss: 0.7114 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6642 - acc: 0.7597 - val_loss: 0.6702 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6655 - acc: 0.7566 - val_loss: 0.7458 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.6519 - acc: 0.7621 - val_loss: 0.7645 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.6418 - acc: 0.7667 - val_loss: 0.7189 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 66us/step - loss: 0.6301 - acc: 0.7585 - val_loss: 0.7140 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.6252 - acc: 0.7725 - val_loss: 0.7212 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 75us/step - loss: 0.6156 - acc: 0.7719 - val_loss: 0.7001 - val_acc: 0.7647\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.6023 - acc: 0.7776 - val_loss: 0.7356 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 70us/step - loss: 0.5788 - acc: 0.7853 - val_loss: 0.7197 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 71us/step - loss: 0.5944 - acc: 0.7871 - val_loss: 0.7146 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.5676 - acc: 0.7959 - val_loss: 0.7459 - val_acc: 0.7059\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.5719 - acc: 0.7917 - val_loss: 0.7265 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 68us/step - loss: 0.5388 - acc: 0.8121 - val_loss: 0.7556 - val_acc: 0.7353\n",
      "Train on 3283 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.5545 - acc: 0.7971 - val_loss: 0.7416 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3283/3283 [==============================] - 0s 67us/step - loss: 0.5477 - acc: 0.8032 - val_loss: 0.7089 - val_acc: 0.7353\n",
      "begin training\n",
      "\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 2s 630us/step - loss: 1.9528 - acc: 0.2485 - val_loss: 1.6213 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 1.7587 - acc: 0.3401 - val_loss: 1.4751 - val_acc: 0.4706\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 66us/step - loss: 1.6467 - acc: 0.3931 - val_loss: 1.3726 - val_acc: 0.4412\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 1.5581 - acc: 0.4287 - val_loss: 1.3242 - val_acc: 0.4706\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 74us/step - loss: 1.4955 - acc: 0.4495 - val_loss: 1.2451 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 1.4451 - acc: 0.4635 - val_loss: 1.2367 - val_acc: 0.4706\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 1.4163 - acc: 0.4723 - val_loss: 1.1778 - val_acc: 0.4412\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 1.3713 - acc: 0.4973 - val_loss: 1.1311 - val_acc: 0.5000\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 1.3429 - acc: 0.5033 - val_loss: 1.1042 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 75us/step - loss: 1.3084 - acc: 0.5137 - val_loss: 1.0571 - val_acc: 0.5294\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 1.2793 - acc: 0.5292 - val_loss: 1.0370 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 1.2487 - acc: 0.5414 - val_loss: 0.9953 - val_acc: 0.5588\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 74us/step - loss: 1.2250 - acc: 0.5624 - val_loss: 0.9888 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 1.1942 - acc: 0.5679 - val_loss: 0.9821 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 1.1728 - acc: 0.5725 - val_loss: 0.9546 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 75us/step - loss: 1.1432 - acc: 0.5789 - val_loss: 0.9296 - val_acc: 0.5882\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 75us/step - loss: 1.1399 - acc: 0.5813 - val_loss: 0.9364 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 1.0972 - acc: 0.6072 - val_loss: 0.9272 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 1.0797 - acc: 0.6102 - val_loss: 0.8907 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 1.0528 - acc: 0.6169 - val_loss: 0.8614 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 1.0399 - acc: 0.6242 - val_loss: 0.8743 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 1.0063 - acc: 0.6382 - val_loss: 0.8530 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.9957 - acc: 0.6340 - val_loss: 0.8390 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.9748 - acc: 0.6510 - val_loss: 0.8453 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.9567 - acc: 0.6605 - val_loss: 0.8554 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 0.9314 - acc: 0.6583 - val_loss: 0.8304 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.9268 - acc: 0.6733 - val_loss: 0.8199 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.9202 - acc: 0.6751 - val_loss: 0.8438 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 73us/step - loss: 0.9060 - acc: 0.6745 - val_loss: 0.8208 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.8848 - acc: 0.6812 - val_loss: 0.8102 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.8711 - acc: 0.6900 - val_loss: 0.7963 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.8414 - acc: 0.7010 - val_loss: 0.8018 - val_acc: 0.6176\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.8337 - acc: 0.7040 - val_loss: 0.8041 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 0.8324 - acc: 0.7001 - val_loss: 0.8244 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 74us/step - loss: 0.8066 - acc: 0.7077 - val_loss: 0.8435 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.8024 - acc: 0.7116 - val_loss: 0.8007 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 79us/step - loss: 0.7783 - acc: 0.7189 - val_loss: 0.8231 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.7719 - acc: 0.7168 - val_loss: 0.7969 - val_acc: 0.7353\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.7517 - acc: 0.7202 - val_loss: 0.8328 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 71us/step - loss: 0.7630 - acc: 0.7238 - val_loss: 0.8024 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.7361 - acc: 0.7326 - val_loss: 0.7898 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.7435 - acc: 0.7284 - val_loss: 0.8114 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.7156 - acc: 0.7305 - val_loss: 0.8297 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 76us/step - loss: 0.7158 - acc: 0.7430 - val_loss: 0.8203 - val_acc: 0.6471\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.7025 - acc: 0.7470 - val_loss: 0.8296 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.6884 - acc: 0.7521 - val_loss: 0.8289 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 80us/step - loss: 0.6742 - acc: 0.7625 - val_loss: 0.7995 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.6766 - acc: 0.7543 - val_loss: 0.8324 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6469 - acc: 0.7613 - val_loss: 0.7996 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 81us/step - loss: 0.6486 - acc: 0.7570 - val_loss: 0.7982 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.6302 - acc: 0.7710 - val_loss: 0.8141 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 0.6290 - acc: 0.7738 - val_loss: 0.7985 - val_acc: 0.7059\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6336 - acc: 0.7616 - val_loss: 0.7702 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 72us/step - loss: 0.6056 - acc: 0.7734 - val_loss: 0.8143 - val_acc: 0.6765\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 70us/step - loss: 0.6008 - acc: 0.7805 - val_loss: 0.8210 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 0.5906 - acc: 0.7887 - val_loss: 0.8178 - val_acc: 0.7353\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 68us/step - loss: 0.5822 - acc: 0.7783 - val_loss: 0.8237 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.5700 - acc: 0.7856 - val_loss: 0.8097 - val_acc: 0.7353\n",
      "Train on 3284 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3284/3284 [==============================] - 0s 77us/step - loss: 0.5576 - acc: 0.7902 - val_loss: 0.8428 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3284/3284 [==============================] - 0s 69us/step - loss: 0.5749 - acc: 0.7893 - val_loss: 0.8539 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 3s 971us/step - loss: 1.9478 - acc: 0.2682 - val_loss: 1.6280 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.7748 - acc: 0.3358 - val_loss: 1.4864 - val_acc: 0.5000\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.6860 - acc: 0.3820 - val_loss: 1.3346 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 1.5720 - acc: 0.4332 - val_loss: 1.2520 - val_acc: 0.5294\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.5019 - acc: 0.4624 - val_loss: 1.2356 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.4493 - acc: 0.4645 - val_loss: 1.1643 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.4168 - acc: 0.4843 - val_loss: 1.1426 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 1.3774 - acc: 0.4922 - val_loss: 1.0945 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 1.3390 - acc: 0.5239 - val_loss: 1.0570 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 1.3032 - acc: 0.5248 - val_loss: 1.0245 - val_acc: 0.5588\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.2900 - acc: 0.5315 - val_loss: 0.9776 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.2320 - acc: 0.5577 - val_loss: 0.9466 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 1.2142 - acc: 0.5723 - val_loss: 0.9309 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.1974 - acc: 0.5741 - val_loss: 0.8901 - val_acc: 0.6471\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.1561 - acc: 0.5945 - val_loss: 0.8678 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.1408 - acc: 0.5915 - val_loss: 0.8552 - val_acc: 0.6176\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.1121 - acc: 0.6073 - val_loss: 0.8297 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0940 - acc: 0.6149 - val_loss: 0.7990 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 70us/step - loss: 1.0796 - acc: 0.6122 - val_loss: 0.7989 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0430 - acc: 0.6301 - val_loss: 0.7819 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 1.0255 - acc: 0.6457 - val_loss: 0.7851 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 1.0025 - acc: 0.6417 - val_loss: 0.7792 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 74us/step - loss: 0.9997 - acc: 0.6460 - val_loss: 0.7906 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.9815 - acc: 0.6594 - val_loss: 0.7745 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.9453 - acc: 0.6755 - val_loss: 0.7371 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.9320 - acc: 0.6749 - val_loss: 0.7527 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.9223 - acc: 0.6843 - val_loss: 0.7277 - val_acc: 0.7353\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.9160 - acc: 0.6721 - val_loss: 0.7333 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.8953 - acc: 0.6852 - val_loss: 0.7447 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.8949 - acc: 0.6816 - val_loss: 0.7204 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.8552 - acc: 0.6913 - val_loss: 0.7005 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 72us/step - loss: 0.8419 - acc: 0.7038 - val_loss: 0.7057 - val_acc: 0.7647\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 71us/step - loss: 0.8332 - acc: 0.7105 - val_loss: 0.7026 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.8230 - acc: 0.7017 - val_loss: 0.7439 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.7902 - acc: 0.7215 - val_loss: 0.7154 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.7974 - acc: 0.7196 - val_loss: 0.7362 - val_acc: 0.6765\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.7834 - acc: 0.7169 - val_loss: 0.7238 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.7735 - acc: 0.7187 - val_loss: 0.7361 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.7531 - acc: 0.7382 - val_loss: 0.7127 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.7446 - acc: 0.7373 - val_loss: 0.7265 - val_acc: 0.7059\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.7226 - acc: 0.7370 - val_loss: 0.6938 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.7102 - acc: 0.7516 - val_loss: 0.7064 - val_acc: 0.7647\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6828 - acc: 0.7553 - val_loss: 0.7382 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6967 - acc: 0.7519 - val_loss: 0.7604 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.6746 - acc: 0.7543 - val_loss: 0.7318 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6661 - acc: 0.7632 - val_loss: 0.7244 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6599 - acc: 0.7577 - val_loss: 0.7274 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6592 - acc: 0.7656 - val_loss: 0.7403 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6222 - acc: 0.7714 - val_loss: 0.7512 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6103 - acc: 0.7766 - val_loss: 0.7280 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.6335 - acc: 0.7656 - val_loss: 0.7316 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6236 - acc: 0.7747 - val_loss: 0.7633 - val_acc: 0.7647\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.6136 - acc: 0.7747 - val_loss: 0.7367 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.5887 - acc: 0.7863 - val_loss: 0.7202 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5795 - acc: 0.7909 - val_loss: 0.7420 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 69us/step - loss: 0.5881 - acc: 0.7878 - val_loss: 0.7667 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5774 - acc: 0.7927 - val_loss: 0.7780 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5645 - acc: 0.7967 - val_loss: 0.7886 - val_acc: 0.7353\n",
      "Train on 3285 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3285/3285 [==============================] - 0s 68us/step - loss: 0.5618 - acc: 0.7896 - val_loss: 0.8140 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3285/3285 [==============================] - 0s 67us/step - loss: 0.5478 - acc: 0.7973 - val_loss: 0.7533 - val_acc: 0.7059\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 2s 662us/step - loss: 1.9457 - acc: 0.2739 - val_loss: 1.5721 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.7861 - acc: 0.3314 - val_loss: 1.4598 - val_acc: 0.5000\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.6689 - acc: 0.3984 - val_loss: 1.3316 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.5660 - acc: 0.4236 - val_loss: 1.2237 - val_acc: 0.5294\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.4799 - acc: 0.4635 - val_loss: 1.1724 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.4311 - acc: 0.4671 - val_loss: 1.0795 - val_acc: 0.5588\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 86us/step - loss: 1.3851 - acc: 0.4988 - val_loss: 1.0681 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.3466 - acc: 0.5052 - val_loss: 1.0144 - val_acc: 0.6471\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 1.3142 - acc: 0.5198 - val_loss: 1.0077 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 1.2731 - acc: 0.5469 - val_loss: 0.9307 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 82us/step - loss: 1.2421 - acc: 0.5542 - val_loss: 0.8764 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 1.2296 - acc: 0.5691 - val_loss: 0.8817 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 79us/step - loss: 1.1814 - acc: 0.5752 - val_loss: 0.8279 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 1.1667 - acc: 0.5910 - val_loss: 0.8371 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 79us/step - loss: 1.1425 - acc: 0.5934 - val_loss: 0.8071 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 84us/step - loss: 1.1277 - acc: 0.5962 - val_loss: 0.8011 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 1.1023 - acc: 0.6099 - val_loss: 0.7955 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 1.0818 - acc: 0.6123 - val_loss: 0.7848 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 1.0506 - acc: 0.6312 - val_loss: 0.8202 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 76us/step - loss: 1.0324 - acc: 0.6312 - val_loss: 0.7829 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 82us/step - loss: 1.0157 - acc: 0.6409 - val_loss: 0.7598 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 107us/step - loss: 0.9931 - acc: 0.6461 - val_loss: 0.6960 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 106us/step - loss: 0.9751 - acc: 0.6506 - val_loss: 0.7126 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 0.9567 - acc: 0.6561 - val_loss: 0.7525 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.9464 - acc: 0.6649 - val_loss: 0.7283 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.9356 - acc: 0.6677 - val_loss: 0.7330 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.9124 - acc: 0.6808 - val_loss: 0.7600 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.9019 - acc: 0.6795 - val_loss: 0.6900 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 93us/step - loss: 0.8881 - acc: 0.6817 - val_loss: 0.7102 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 91us/step - loss: 0.8653 - acc: 0.6820 - val_loss: 0.7423 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.8353 - acc: 0.6990 - val_loss: 0.6731 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.8456 - acc: 0.7030 - val_loss: 0.6591 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 0.8126 - acc: 0.7039 - val_loss: 0.6693 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 96us/step - loss: 0.8126 - acc: 0.7149 - val_loss: 0.6649 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.7921 - acc: 0.7118 - val_loss: 0.6699 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7755 - acc: 0.7292 - val_loss: 0.7286 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.7751 - acc: 0.7234 - val_loss: 0.7010 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7603 - acc: 0.7264 - val_loss: 0.6831 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7427 - acc: 0.7358 - val_loss: 0.6530 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 87us/step - loss: 0.7355 - acc: 0.7368 - val_loss: 0.6795 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.7191 - acc: 0.7404 - val_loss: 0.6984 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7116 - acc: 0.7447 - val_loss: 0.6849 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6899 - acc: 0.7575 - val_loss: 0.6710 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6908 - acc: 0.7562 - val_loss: 0.6918 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.6726 - acc: 0.7544 - val_loss: 0.6915 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 84us/step - loss: 0.6809 - acc: 0.7587 - val_loss: 0.6918 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6556 - acc: 0.7611 - val_loss: 0.6661 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.6586 - acc: 0.7596 - val_loss: 0.6969 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.6435 - acc: 0.7718 - val_loss: 0.7170 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6447 - acc: 0.7696 - val_loss: 0.6990 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.6255 - acc: 0.7757 - val_loss: 0.6588 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6089 - acc: 0.7803 - val_loss: 0.6629 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.5984 - acc: 0.7812 - val_loss: 0.6474 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 0.5996 - acc: 0.7730 - val_loss: 0.7312 - val_acc: 0.8235\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.5857 - acc: 0.7870 - val_loss: 0.6785 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.5904 - acc: 0.7845 - val_loss: 0.6885 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5600 - acc: 0.7952 - val_loss: 0.6889 - val_acc: 0.8235\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5498 - acc: 0.8022 - val_loss: 0.7400 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.5604 - acc: 0.8019 - val_loss: 0.6697 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 107us/step - loss: 0.5533 - acc: 0.8010 - val_loss: 0.6817 - val_acc: 0.7941\n",
      "begin training\n",
      "\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 2s 668us/step - loss: 1.9402 - acc: 0.2775 - val_loss: 1.5331 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 1.7572 - acc: 0.3460 - val_loss: 1.3630 - val_acc: 0.5588\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 1.6182 - acc: 0.4087 - val_loss: 1.2332 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 89us/step - loss: 1.5206 - acc: 0.4437 - val_loss: 1.1385 - val_acc: 0.5588\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 81us/step - loss: 1.4686 - acc: 0.4617 - val_loss: 1.0766 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 90us/step - loss: 1.4175 - acc: 0.4766 - val_loss: 1.0428 - val_acc: 0.5882\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 89us/step - loss: 1.3867 - acc: 0.4933 - val_loss: 0.9904 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 94us/step - loss: 1.3367 - acc: 0.5088 - val_loss: 0.9449 - val_acc: 0.5882\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 86us/step - loss: 1.2989 - acc: 0.5222 - val_loss: 0.8825 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.2636 - acc: 0.5408 - val_loss: 0.8800 - val_acc: 0.6176\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 1.2365 - acc: 0.5575 - val_loss: 0.8592 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3286/3286 [==============================] - 0s 97us/step - loss: 1.2075 - acc: 0.5560 - val_loss: 0.8173 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 1.1719 - acc: 0.5822 - val_loss: 0.8065 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 74us/step - loss: 1.1550 - acc: 0.5794 - val_loss: 0.7748 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 1.1032 - acc: 0.5986 - val_loss: 0.7628 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 1.1102 - acc: 0.6074 - val_loss: 0.7581 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 83us/step - loss: 1.0575 - acc: 0.6223 - val_loss: 0.7277 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 1.0491 - acc: 0.6223 - val_loss: 0.7411 - val_acc: 0.7941\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 1.0383 - acc: 0.6333 - val_loss: 0.6975 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 1.0129 - acc: 0.6376 - val_loss: 0.7038 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 0.9940 - acc: 0.6442 - val_loss: 0.7021 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.9617 - acc: 0.6625 - val_loss: 0.6972 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 76us/step - loss: 0.9538 - acc: 0.6564 - val_loss: 0.6772 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.9367 - acc: 0.6692 - val_loss: 0.6953 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.9208 - acc: 0.6707 - val_loss: 0.7180 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.9100 - acc: 0.6799 - val_loss: 0.6928 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.8985 - acc: 0.6762 - val_loss: 0.6847 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 77us/step - loss: 0.8724 - acc: 0.6905 - val_loss: 0.6854 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.8593 - acc: 0.6984 - val_loss: 0.6722 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 0.8371 - acc: 0.7030 - val_loss: 0.6717 - val_acc: 0.7059\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.8278 - acc: 0.7036 - val_loss: 0.6486 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.8007 - acc: 0.7106 - val_loss: 0.6332 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.8003 - acc: 0.7097 - val_loss: 0.6175 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7832 - acc: 0.7167 - val_loss: 0.6526 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.7622 - acc: 0.7194 - val_loss: 0.7055 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.7687 - acc: 0.7234 - val_loss: 0.7100 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.7470 - acc: 0.7222 - val_loss: 0.6647 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.7394 - acc: 0.7328 - val_loss: 0.6543 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.7146 - acc: 0.7471 - val_loss: 0.6560 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.7066 - acc: 0.7419 - val_loss: 0.7053 - val_acc: 0.6765\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6999 - acc: 0.7453 - val_loss: 0.6824 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6805 - acc: 0.7593 - val_loss: 0.6413 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 69us/step - loss: 0.6914 - acc: 0.7486 - val_loss: 0.6461 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 70us/step - loss: 0.6650 - acc: 0.7572 - val_loss: 0.5929 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6433 - acc: 0.7721 - val_loss: 0.6297 - val_acc: 0.7941\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.6528 - acc: 0.7638 - val_loss: 0.6049 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6370 - acc: 0.7684 - val_loss: 0.6761 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 67us/step - loss: 0.6202 - acc: 0.7800 - val_loss: 0.6434 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6275 - acc: 0.7727 - val_loss: 0.6964 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 72us/step - loss: 0.6096 - acc: 0.7827 - val_loss: 0.6262 - val_acc: 0.7353\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6022 - acc: 0.7812 - val_loss: 0.6438 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 68us/step - loss: 0.6012 - acc: 0.7845 - val_loss: 0.6567 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5978 - acc: 0.7845 - val_loss: 0.6742 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.5707 - acc: 0.7934 - val_loss: 0.6707 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5812 - acc: 0.7839 - val_loss: 0.6886 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 75us/step - loss: 0.5612 - acc: 0.7979 - val_loss: 0.6380 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 71us/step - loss: 0.5649 - acc: 0.7943 - val_loss: 0.7472 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5391 - acc: 0.8019 - val_loss: 0.7203 - val_acc: 0.7647\n",
      "Train on 3286 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3286/3286 [==============================] - 0s 73us/step - loss: 0.5381 - acc: 0.8040 - val_loss: 0.6816 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3286/3286 [==============================] - 0s 78us/step - loss: 0.5515 - acc: 0.7967 - val_loss: 0.6501 - val_acc: 0.7941\n",
      "begin training\n",
      "\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 2s 670us/step - loss: 1.9501 - acc: 0.2565 - val_loss: 1.5901 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 77us/step - loss: 1.7554 - acc: 0.3553 - val_loss: 1.4193 - val_acc: 0.4706\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 1.6411 - acc: 0.4083 - val_loss: 1.2950 - val_acc: 0.5000\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 70us/step - loss: 1.5460 - acc: 0.4378 - val_loss: 1.2648 - val_acc: 0.5294\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.5023 - acc: 0.4460 - val_loss: 1.2143 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.4491 - acc: 0.4734 - val_loss: 1.1967 - val_acc: 0.6176\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 1.3939 - acc: 0.4880 - val_loss: 1.1187 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 91us/step - loss: 1.3741 - acc: 0.4965 - val_loss: 1.0653 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 1.3332 - acc: 0.5075 - val_loss: 1.0167 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 78us/step - loss: 1.3004 - acc: 0.5172 - val_loss: 0.9939 - val_acc: 0.5588\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 1.2552 - acc: 0.5391 - val_loss: 0.9803 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 75us/step - loss: 1.2317 - acc: 0.5519 - val_loss: 0.9048 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 1.2090 - acc: 0.5604 - val_loss: 0.9247 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 1.1765 - acc: 0.5789 - val_loss: 0.9179 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 1.1546 - acc: 0.5750 - val_loss: 0.8568 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.1361 - acc: 0.5966 - val_loss: 0.8326 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 76us/step - loss: 1.1145 - acc: 0.5963 - val_loss: 0.8586 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 1.0887 - acc: 0.6124 - val_loss: 0.8305 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 95us/step - loss: 1.0611 - acc: 0.6255 - val_loss: 0.7827 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 1.0359 - acc: 0.6255 - val_loss: 0.7869 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 1.0309 - acc: 0.6276 - val_loss: 0.7982 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 1.0073 - acc: 0.6392 - val_loss: 0.7865 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.9883 - acc: 0.6538 - val_loss: 0.7794 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.9640 - acc: 0.6504 - val_loss: 0.7600 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.9541 - acc: 0.6571 - val_loss: 0.7606 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 78us/step - loss: 0.9215 - acc: 0.6644 - val_loss: 0.7434 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.9149 - acc: 0.6681 - val_loss: 0.7689 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 0.9022 - acc: 0.6812 - val_loss: 0.7610 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.8933 - acc: 0.6681 - val_loss: 0.7669 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.8637 - acc: 0.6897 - val_loss: 0.7473 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.8455 - acc: 0.7009 - val_loss: 0.7532 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.8367 - acc: 0.6933 - val_loss: 0.7291 - val_acc: 0.6765\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 84us/step - loss: 0.8272 - acc: 0.7082 - val_loss: 0.7218 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 81us/step - loss: 0.7986 - acc: 0.7122 - val_loss: 0.7230 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 88us/step - loss: 0.7897 - acc: 0.7131 - val_loss: 0.7600 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.7727 - acc: 0.7301 - val_loss: 0.7317 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 81us/step - loss: 0.7775 - acc: 0.7162 - val_loss: 0.7375 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 79us/step - loss: 0.7529 - acc: 0.7228 - val_loss: 0.7433 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.7354 - acc: 0.7362 - val_loss: 0.7467 - val_acc: 0.7647\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 70us/step - loss: 0.7271 - acc: 0.7448 - val_loss: 0.7711 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 74us/step - loss: 0.7253 - acc: 0.7350 - val_loss: 0.7631 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 88us/step - loss: 0.7021 - acc: 0.7451 - val_loss: 0.7900 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 72us/step - loss: 0.6955 - acc: 0.7539 - val_loss: 0.7971 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6946 - acc: 0.7493 - val_loss: 0.8301 - val_acc: 0.6471\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6826 - acc: 0.7530 - val_loss: 0.7795 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6516 - acc: 0.7597 - val_loss: 0.8088 - val_acc: 0.7353\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6517 - acc: 0.7591 - val_loss: 0.7869 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.6342 - acc: 0.7612 - val_loss: 0.8255 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.6238 - acc: 0.7673 - val_loss: 0.8297 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 73us/step - loss: 0.6295 - acc: 0.7651 - val_loss: 0.8310 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.6254 - acc: 0.7709 - val_loss: 0.8198 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.6083 - acc: 0.7764 - val_loss: 0.8633 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 71us/step - loss: 0.5987 - acc: 0.7828 - val_loss: 0.7878 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 68us/step - loss: 0.5896 - acc: 0.7916 - val_loss: 0.8428 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.5873 - acc: 0.7810 - val_loss: 0.7961 - val_acc: 0.7353\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.5818 - acc: 0.7864 - val_loss: 0.8590 - val_acc: 0.7059\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.5603 - acc: 0.7892 - val_loss: 0.8571 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.5712 - acc: 0.7959 - val_loss: 0.8403 - val_acc: 0.7647\n",
      "Train on 3287 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3287/3287 [==============================] - 0s 69us/step - loss: 0.5583 - acc: 0.7965 - val_loss: 0.8313 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3287/3287 [==============================] - 0s 67us/step - loss: 0.5410 - acc: 0.8010 - val_loss: 0.8404 - val_acc: 0.7353\n",
      "begin training\n",
      "\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 2s 681us/step - loss: 1.9716 - acc: 0.2475 - val_loss: 1.5614 - val_acc: 0.4706\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.7834 - acc: 0.3305 - val_loss: 1.3932 - val_acc: 0.5294\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.6623 - acc: 0.3953 - val_loss: 1.1995 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.5738 - acc: 0.4159 - val_loss: 1.1888 - val_acc: 0.5000\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.5091 - acc: 0.4427 - val_loss: 1.0702 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 1.4451 - acc: 0.4625 - val_loss: 1.0124 - val_acc: 0.5588\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 1.4116 - acc: 0.4819 - val_loss: 0.9775 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 68us/step - loss: 1.3663 - acc: 0.4962 - val_loss: 0.9461 - val_acc: 0.5882\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 1.3250 - acc: 0.5105 - val_loss: 0.9078 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 1.2962 - acc: 0.5254 - val_loss: 0.8556 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 77us/step - loss: 1.2531 - acc: 0.5445 - val_loss: 0.8360 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 1.2286 - acc: 0.5537 - val_loss: 0.8278 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 1.1997 - acc: 0.5582 - val_loss: 0.8050 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 95us/step - loss: 1.1774 - acc: 0.5841 - val_loss: 0.8139 - val_acc: 0.7353\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 89us/step - loss: 1.1344 - acc: 0.5947 - val_loss: 0.7828 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 88us/step - loss: 1.1162 - acc: 0.5956 - val_loss: 0.7199 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 1.1062 - acc: 0.6005 - val_loss: 0.7275 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 1.0754 - acc: 0.6193 - val_loss: 0.7204 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 1.0477 - acc: 0.6263 - val_loss: 0.7230 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 1.0311 - acc: 0.6242 - val_loss: 0.7116 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 1.0214 - acc: 0.6291 - val_loss: 0.7055 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.9927 - acc: 0.6379 - val_loss: 0.6761 - val_acc: 0.7353\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 90us/step - loss: 0.9696 - acc: 0.6531 - val_loss: 0.7007 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.9600 - acc: 0.6607 - val_loss: 0.7176 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.9439 - acc: 0.6564 - val_loss: 0.6924 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.9222 - acc: 0.6686 - val_loss: 0.6857 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.8973 - acc: 0.6780 - val_loss: 0.6716 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.8851 - acc: 0.6847 - val_loss: 0.6529 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.8780 - acc: 0.6850 - val_loss: 0.6933 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.8553 - acc: 0.6969 - val_loss: 0.6584 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 73us/step - loss: 0.8537 - acc: 0.6941 - val_loss: 0.6579 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 79us/step - loss: 0.8221 - acc: 0.7145 - val_loss: 0.6852 - val_acc: 0.7353\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.8195 - acc: 0.7127 - val_loss: 0.6598 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 93us/step - loss: 0.7960 - acc: 0.7178 - val_loss: 0.6516 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.7881 - acc: 0.7182 - val_loss: 0.6725 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 77us/step - loss: 0.7899 - acc: 0.7175 - val_loss: 0.6510 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.7741 - acc: 0.7200 - val_loss: 0.6445 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.7714 - acc: 0.7279 - val_loss: 0.6691 - val_acc: 0.6176\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.7460 - acc: 0.7327 - val_loss: 0.6468 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.7356 - acc: 0.7370 - val_loss: 0.6666 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.7280 - acc: 0.7285 - val_loss: 0.6340 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.6977 - acc: 0.7489 - val_loss: 0.6574 - val_acc: 0.6471\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.6862 - acc: 0.7464 - val_loss: 0.6316 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 76us/step - loss: 0.6834 - acc: 0.7492 - val_loss: 0.6330 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 77us/step - loss: 0.6680 - acc: 0.7613 - val_loss: 0.6376 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 76us/step - loss: 0.6765 - acc: 0.7580 - val_loss: 0.6538 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.6595 - acc: 0.7610 - val_loss: 0.6213 - val_acc: 0.6471\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3289/3289 [==============================] - 0s 75us/step - loss: 0.6465 - acc: 0.7598 - val_loss: 0.7222 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.6361 - acc: 0.7732 - val_loss: 0.6821 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.6184 - acc: 0.7738 - val_loss: 0.6548 - val_acc: 0.6765\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 74us/step - loss: 0.6042 - acc: 0.7805 - val_loss: 0.6566 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.6110 - acc: 0.7738 - val_loss: 0.6286 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 78us/step - loss: 0.6098 - acc: 0.7750 - val_loss: 0.6618 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 71us/step - loss: 0.5904 - acc: 0.7796 - val_loss: 0.6262 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 69us/step - loss: 0.5869 - acc: 0.7850 - val_loss: 0.6650 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 67us/step - loss: 0.5760 - acc: 0.7869 - val_loss: 0.6401 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.5532 - acc: 0.7945 - val_loss: 0.6606 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 72us/step - loss: 0.5621 - acc: 0.7939 - val_loss: 0.7159 - val_acc: 0.7059\n",
      "Train on 3289 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3289/3289 [==============================] - 0s 70us/step - loss: 0.5474 - acc: 0.7987 - val_loss: 0.6580 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3289/3289 [==============================] - 0s 89us/step - loss: 0.5463 - acc: 0.8002 - val_loss: 0.6818 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 715us/step - loss: 1.9346 - acc: 0.2611 - val_loss: 1.6339 - val_acc: 0.3529\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.7783 - acc: 0.3295 - val_loss: 1.5242 - val_acc: 0.3824\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.6691 - acc: 0.3851 - val_loss: 1.3715 - val_acc: 0.4118\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.5664 - acc: 0.4316 - val_loss: 1.2917 - val_acc: 0.4706\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.4912 - acc: 0.4395 - val_loss: 1.2358 - val_acc: 0.4412\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.4435 - acc: 0.4617 - val_loss: 1.1825 - val_acc: 0.4412\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.3950 - acc: 0.4796 - val_loss: 1.1638 - val_acc: 0.5000\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.3593 - acc: 0.4979 - val_loss: 1.1030 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.3257 - acc: 0.5131 - val_loss: 1.0730 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.2749 - acc: 0.5283 - val_loss: 1.0434 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 1.2397 - acc: 0.5447 - val_loss: 1.0129 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.2242 - acc: 0.5632 - val_loss: 0.9505 - val_acc: 0.5294\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.1999 - acc: 0.5599 - val_loss: 0.9709 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.1476 - acc: 0.5897 - val_loss: 0.9236 - val_acc: 0.5294\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.1392 - acc: 0.5897 - val_loss: 0.9048 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 1.1191 - acc: 0.5960 - val_loss: 0.9157 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 1.1013 - acc: 0.6103 - val_loss: 0.8750 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 90us/step - loss: 1.0800 - acc: 0.6152 - val_loss: 0.8679 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.0524 - acc: 0.6328 - val_loss: 0.8465 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 1.0268 - acc: 0.6274 - val_loss: 0.8267 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.0117 - acc: 0.6416 - val_loss: 0.8080 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 1.0072 - acc: 0.6426 - val_loss: 0.8015 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.9775 - acc: 0.6584 - val_loss: 0.7904 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.9539 - acc: 0.6587 - val_loss: 0.7789 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.9323 - acc: 0.6660 - val_loss: 0.7750 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.9373 - acc: 0.6684 - val_loss: 0.7401 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9049 - acc: 0.6736 - val_loss: 0.7730 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.9067 - acc: 0.6729 - val_loss: 0.7248 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.8601 - acc: 0.6909 - val_loss: 0.7396 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8682 - acc: 0.6939 - val_loss: 0.7220 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.8424 - acc: 0.7079 - val_loss: 0.7109 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.8236 - acc: 0.7021 - val_loss: 0.6734 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.8221 - acc: 0.7055 - val_loss: 0.6957 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.7983 - acc: 0.7146 - val_loss: 0.7296 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.7806 - acc: 0.7216 - val_loss: 0.6780 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7691 - acc: 0.7240 - val_loss: 0.7013 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7601 - acc: 0.7277 - val_loss: 0.6640 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.7532 - acc: 0.7298 - val_loss: 0.6936 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.7307 - acc: 0.7416 - val_loss: 0.7032 - val_acc: 0.7059\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7233 - acc: 0.7435 - val_loss: 0.7140 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.7156 - acc: 0.7422 - val_loss: 0.7025 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7077 - acc: 0.7407 - val_loss: 0.7076 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.7058 - acc: 0.7422 - val_loss: 0.6951 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 0.6762 - acc: 0.7541 - val_loss: 0.7281 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.6648 - acc: 0.7520 - val_loss: 0.6847 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6637 - acc: 0.7587 - val_loss: 0.7015 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6634 - acc: 0.7626 - val_loss: 0.6777 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6416 - acc: 0.7690 - val_loss: 0.6751 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.6287 - acc: 0.7729 - val_loss: 0.7066 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.6111 - acc: 0.7751 - val_loss: 0.6638 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.6173 - acc: 0.7772 - val_loss: 0.6991 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.6191 - acc: 0.7754 - val_loss: 0.7121 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 68us/step - loss: 0.5972 - acc: 0.7827 - val_loss: 0.7180 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5783 - acc: 0.7812 - val_loss: 0.6599 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.5974 - acc: 0.7781 - val_loss: 0.6759 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 93us/step - loss: 0.5771 - acc: 0.7985 - val_loss: 0.6791 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5713 - acc: 0.7881 - val_loss: 0.6812 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.5437 - acc: 0.8024 - val_loss: 0.6987 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 71us/step - loss: 0.5566 - acc: 0.7988 - val_loss: 0.6945 - val_acc: 0.5882\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.5378 - acc: 0.8015 - val_loss: 0.7002 - val_acc: 0.6765\n",
      "begin training\n",
      "\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 2s 715us/step - loss: 1.9620 - acc: 0.2441 - val_loss: 1.5580 - val_acc: 0.3824\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 1.7591 - acc: 0.3316 - val_loss: 1.4168 - val_acc: 0.4412\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.6357 - acc: 0.3973 - val_loss: 1.3636 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.5480 - acc: 0.4325 - val_loss: 1.2942 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.5056 - acc: 0.4483 - val_loss: 1.2821 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 1.4552 - acc: 0.4562 - val_loss: 1.1900 - val_acc: 0.5000\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.4118 - acc: 0.4836 - val_loss: 1.1314 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.3868 - acc: 0.4900 - val_loss: 1.0711 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.3550 - acc: 0.5021 - val_loss: 1.0389 - val_acc: 0.5588\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.3170 - acc: 0.5204 - val_loss: 1.0047 - val_acc: 0.5588\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 1.2767 - acc: 0.5371 - val_loss: 0.9817 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.2459 - acc: 0.5541 - val_loss: 0.9434 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 94us/step - loss: 1.2168 - acc: 0.5620 - val_loss: 0.9119 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 1.2072 - acc: 0.5578 - val_loss: 0.8872 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 1.1685 - acc: 0.5711 - val_loss: 0.8490 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 1.1416 - acc: 0.5933 - val_loss: 0.8446 - val_acc: 0.5882\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 82us/step - loss: 1.1320 - acc: 0.5967 - val_loss: 0.8013 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 1.1034 - acc: 0.6073 - val_loss: 0.7990 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 92us/step - loss: 1.0693 - acc: 0.6146 - val_loss: 0.7962 - val_acc: 0.6176\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.0370 - acc: 0.6264 - val_loss: 0.7830 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 1.0346 - acc: 0.6292 - val_loss: 0.7719 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 70us/step - loss: 1.0142 - acc: 0.6447 - val_loss: 0.7752 - val_acc: 0.6471\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9834 - acc: 0.6495 - val_loss: 0.7810 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.9627 - acc: 0.6663 - val_loss: 0.7534 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.9569 - acc: 0.6596 - val_loss: 0.7549 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 69us/step - loss: 0.9275 - acc: 0.6644 - val_loss: 0.7490 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 72us/step - loss: 0.9156 - acc: 0.6790 - val_loss: 0.7486 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.8928 - acc: 0.6799 - val_loss: 0.7211 - val_acc: 0.6176\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.8786 - acc: 0.6854 - val_loss: 0.7170 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.8697 - acc: 0.6927 - val_loss: 0.7693 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 83us/step - loss: 0.8592 - acc: 0.6909 - val_loss: 0.6962 - val_acc: 0.6765\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.8375 - acc: 0.6933 - val_loss: 0.7076 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.8225 - acc: 0.7009 - val_loss: 0.6782 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.8067 - acc: 0.7131 - val_loss: 0.6744 - val_acc: 0.7647\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.7992 - acc: 0.7094 - val_loss: 0.7167 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.7909 - acc: 0.7073 - val_loss: 0.6871 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.7533 - acc: 0.7331 - val_loss: 0.7348 - val_acc: 0.6471\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.7558 - acc: 0.7207 - val_loss: 0.7188 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.7489 - acc: 0.7207 - val_loss: 0.7280 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.7190 - acc: 0.7350 - val_loss: 0.7327 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.7231 - acc: 0.7392 - val_loss: 0.7254 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.7157 - acc: 0.7404 - val_loss: 0.7443 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6922 - acc: 0.7480 - val_loss: 0.7372 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6852 - acc: 0.7559 - val_loss: 0.7326 - val_acc: 0.7647\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 77us/step - loss: 0.6789 - acc: 0.7574 - val_loss: 0.7586 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6772 - acc: 0.7571 - val_loss: 0.7897 - val_acc: 0.7353\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.6517 - acc: 0.7590 - val_loss: 0.7402 - val_acc: 0.6765\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6365 - acc: 0.7675 - val_loss: 0.7195 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6194 - acc: 0.7827 - val_loss: 0.7074 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.6330 - acc: 0.7736 - val_loss: 0.7140 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6159 - acc: 0.7742 - val_loss: 0.6799 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 75us/step - loss: 0.6019 - acc: 0.7830 - val_loss: 0.7409 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 84us/step - loss: 0.5888 - acc: 0.7860 - val_loss: 0.7219 - val_acc: 0.7353\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 76us/step - loss: 0.5725 - acc: 0.8033 - val_loss: 0.7313 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 73us/step - loss: 0.5759 - acc: 0.7957 - val_loss: 0.7344 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 78us/step - loss: 0.5751 - acc: 0.7891 - val_loss: 0.8438 - val_acc: 0.6765\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 79us/step - loss: 0.5681 - acc: 0.7942 - val_loss: 0.7738 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 82us/step - loss: 0.5557 - acc: 0.8018 - val_loss: 0.7649 - val_acc: 0.7059\n",
      "Train on 3290 samples, validate on 34 samples\n",
      "Epoch 1/2\n",
      "3290/3290 [==============================] - 0s 74us/step - loss: 0.5417 - acc: 0.8000 - val_loss: 0.7322 - val_acc: 0.7059\n",
      "Epoch 2/2\n",
      "3290/3290 [==============================] - 0s 80us/step - loss: 0.5332 - acc: 0.8125 - val_loss: 0.7959 - val_acc: 0.7353\n",
      "Accuracy: 0.6301 ± 0.0256\n",
      "NMI: 0.3988 ± 0.0338\n",
      "Log_loss: 1.2779 ± 0.1272\n",
      "Normalized confusion matrix\n",
      "[[ 0.60725076  0.02265861  0.00302115  0.20090634  0.07401813  0.03172205\n",
      "   0.05589124  0.          0.00453172]\n",
      " [ 0.06024096  0.46987952  0.00200803  0.03012048  0.01405622  0.02409639\n",
      "   0.39759036  0.          0.00200803]\n",
      " [ 0.09375     0.01041667  0.3125      0.16666667  0.07291667  0.          0.34375\n",
      "   0.          0.        ]\n",
      " [ 0.1930759   0.02130493  0.00932091  0.68175766  0.04394141  0.01065246\n",
      "   0.03994674  0.          0.        ]\n",
      " [ 0.21722846  0.02996255  0.01872659  0.13108614  0.3670412   0.06741573\n",
      "   0.16853933  0.          0.        ]\n",
      " [ 0.11447811  0.04713805  0.00673401  0.06734007  0.06060606  0.59259259\n",
      "   0.11111111  0.          0.        ]\n",
      " [ 0.0227704   0.12049336  0.00948767  0.0199241   0.01612903  0.01043643\n",
      "   0.79696395  0.00094877  0.0028463 ]\n",
      " [ 0.0952381   0.19047619  0.          0.04761905  0.          0.04761905\n",
      "   0.33333333  0.14285714  0.14285714]\n",
      " [ 0.02325581  0.13953488  0.          0.04651163  0.          0.\n",
      "   0.11627907  0.          0.6744186 ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAEgCAYAAADWs+oEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a5b5349b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in modelName:\n",
    "    text_model = wel.loadTextModel(PATH + model)\n",
    "    text_train_arrays, text_test_arrays = wel.getTextVec(text_model, train_size, test_size, TEXT_INPUT_DIM)\n",
    "    train_set = np.hstack((truncated_one_hot_gene[:train_size], truncated_one_hot_variation[:train_size], text_train_arrays))\n",
    "    test_set = np.hstack((truncated_one_hot_gene[train_size:], truncated_one_hot_variation[train_size:], text_test_arrays))\n",
    "    X_train = np.array(train_set)\n",
    "    X_test = np.array(test_set)\n",
    "    print(\"Use test model: \" + model)\n",
    "    nn_cross_validation(X_train, y, encoded_y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = NN_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_predict, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[  6.42123878e-01   5.68419462e-03   1.20462831e-02   1.99772403e-01\n",
      "    9.59234685e-02   3.07848640e-02   1.22579243e-02   9.33274277e-04\n",
      "    4.73716063e-04]\n",
      " [  6.08799160e-01   1.06558669e-03   1.64968206e-03   3.72251213e-01\n",
      "    5.34702698e-03   8.07844941e-03   1.82031468e-03   6.98966149e-04\n",
      "    2.89670046e-04]\n",
      " [  6.00796053e-03   8.34848583e-02   1.99606200e-03   3.64352087e-03\n",
      "    1.85364590e-03   8.96764628e-04   8.98898959e-01   2.72210105e-03\n",
      "    4.96144639e-04]\n",
      " [  2.06863612e-01   3.05877507e-01   1.77739968e-03   7.73829818e-02\n",
      "    6.20944658e-03   8.73012375e-03   3.59056264e-01   2.36274302e-02\n",
      "    1.04752388e-02]\n",
      " [  3.32952011e-03   1.46441370e-01   7.98771251e-03   6.49681687e-03\n",
      "    1.07387956e-02   2.05379468e-03   8.17908585e-01   3.78938811e-03\n",
      "    1.25404028e-03]\n",
      " [  8.63160849e-01   4.18370180e-02   2.00103023e-04   3.03073525e-02\n",
      "    4.05042898e-03   1.09452149e-02   4.10042815e-02   6.02450036e-03\n",
      "    2.47023441e-03]\n",
      " [  2.64032464e-03   1.98983386e-01   1.94788410e-03   3.74622876e-03\n",
      "    4.73850034e-03   3.64766712e-03   7.82235265e-01   1.64504291e-03\n",
      "    4.15707706e-04]\n",
      " [  7.48761177e-01   3.32682440e-03   1.06974412e-03   1.99660480e-01\n",
      "    1.12955282e-02   3.04761883e-02   3.57829058e-03   1.18244113e-03\n",
      "    6.49433699e-04]\n",
      " [  3.09546500e-01   5.40287525e-04   8.07594799e-04   6.79725885e-01\n",
      "    4.64249449e-03   3.89145804e-03   6.10720890e-04   1.63608376e-04\n",
      "    7.13698319e-05]\n",
      " [  6.03528577e-04   1.98140386e-02   1.32115215e-01   4.48965281e-03\n",
      "    2.67964937e-02   2.70279427e-03   8.10846448e-01   1.76985003e-03\n",
      "    8.61989858e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(type(y_pred))\n",
    "print(y_pred_proba[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
